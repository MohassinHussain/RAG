{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38869ef0",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data ingestion to Vector DB Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71bf2ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aa26c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: {pdf_file.name}\n",
      " ✓ Loaded 2 pages\n",
      "\n",
      "Processing: {pdf_file.name}\n",
      " ✓ Loaded 11 pages\n",
      "\n",
      "Processing: {pdf_file.name}\n",
      " ✓ Loaded 16 pages\n",
      "\n",
      "Total documents loaded: 29\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = [ ]\n",
    "    pdf_dir = Path (pdf_directory)\n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list (pdf_dir.glob (\"**/*.pdf\"))\n",
    "    print (f\"Found {len (pdf_files)} PDF files to process\")\n",
    "    for pdf_file in pdf_files:\n",
    "        print(\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            all_documents.extend(documents)\n",
    "            print (f\" ✓ Loaded {len (documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print (f\" X Error: {e}\")\n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c67e6073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content=\"Hello! How can I assist you today?\\nWhen resetting Windows 11 in a dual-boot system with Linux, the Windows reset process may\\noverwrite the bootloader, which can remove or interfere with the GRUB bootloader that manages\\nLinux boot. To avoid losing GRUB when resetting Windows 11, consider these key points:\\nIn summary, resetting Windows 11 alone tends to remove GRUB, but if you are prepared to\\nrestore GRUB using a Linux Live USB afterward, the dual boot can be recovered without data\\nloss on Linux. Carefully manage BIOS boot priorities and be ready to reinstall GRUB if needed.\\nIf preferred, detailed steps for restoring GRUB after a Windows reset or installation are available,\\nincluding mounting the Linux partition and running sudo grub-install commands from a Live\\nenvironment. Would you like step-by-step instructions for restoring GRUB after resetting\\nWindows?\\n⁂\\nHi\\nI have windows 11 and linux I want to reset the\\nwindows and I dont want the grub to be removed\\nBefore the reset, ensure you have a Linux Live USB or rescue media ready, so you can\\nrestore GRUB if it's overwritten. Windows reset typically replaces GRUB with the Windows\\nbootloader, removing the Linux boot option.\\nAfter resetting Windows, if GRUB is missing, it can be restored using a Linux Live\\nenvironment by reinstalling GRUB with commands like grub-install and update-grub\\ntargeted at your Linux partition.\\nAlternatively, checking BIOS/UEFI boot order before and after the reset to ensure the Linux\\nbootloader remains prioritized can prevent Windows from automatically booting and\\nbypassing GRUB.\\nDisabling Windows fast startup before the reset reduces potential interference with the\\nbootloader.\\nSome users suggest setting the Linux disk as primary boot device in BIOS/UEFI, so the\\nGRUB loader controls the boot process, even if Windows modifies its own bootloader.\\n\\x001\\x00\\n\\x002\\x00\\x003\\x00\\n\\x004\\x00\\x001\\x00\\n\\x00\\x00\\x00https://pq.hosting/en/help/restoring-grub-after-installing-windows-10\\n\\x00\\x00\\x00https://www.reddit.com/r/linux4noobs/comments/w6qtrx/will_the_windows_11_update_break_dual_boot/\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content='\\x00\\x00\\x00https://discussion.fedoraproject.org/t/repair-reinstall-grub-after-windows-11-update-dual-boot-fedora\\n-f39/113155\\n\\x00\\x00\\x00https://www.cybrary.it/blog/restoring-grub-installing-windows\\n\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/4166685/(solved)-accidentally-deleted-windows-1\\n1-microsoft?forum=windows-all\\n\\x00\\x00\\x00https://www.reddit.com/r/linuxmint/comments/1fmn07i/reinstalling_windows_wiped_my_grub_but_now_i\\n_cant/\\n\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/3957734/why-isnt-my-windows-booting-after-du\\nal-booting-lin\\n\\x00\\x00\\x00https://forum.manjaro.org/t/remove-grub-and-revert-to-windows-bootloader/103716\\n\\x00\\x00\\x00https://www.youtube.com/watch?v=PVxDttc2z-I\\n\\x00\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/4115492/i-deleted-a-partition-from-windows-11-th\\nat-had-lin\\n\\x00\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/2796971/how-to-restore-windows-bootloader-fro\\nm-grub?forum=windows-all\\n\\x00\\x00\\x00\\x00https://wiki.archlinux.org/title/Dual_boot_with_Windows\\n\\x00\\x00\\x00\\x00https://discourse.ubuntu.com/t/grub-install-issues-on-laptop-with-win11/52702\\n\\x00\\x00\\x00\\x00https://www.reddit.com/r/linuxmint/comments/1hawohz/how_to_restore_window_11_bootloader_no_longe\\nr/\\n\\x00\\x00\\x00\\x00https://www.youtube.com/watch?v=I_1fk15QQ_M\\n\\x00\\x00\\x00\\x00https://www.youtube.com/watch?v=KWVte9WGxGE\\n\\x00\\x00\\x00\\x00https://forum.endeavouros.com/t/no-grub-after-windows-10-install-how-to-recover/45977\\n\\x00\\x00\\x00\\x00https://www.onlogic.com/blog/how-to-dual-boot-windows-11-and-linux/'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2347] \\nEXPLORING TECH FOUNDATIONS: DSA, OOP, DBMS, OS IN  \\nCOMPUTER SCIENCE \\nSushmita C. Hubli*1 \\n*1Department Of Electronics And Telecommunication, Pune Institute Of Computer Technology,  \\nPune, India. \\nDOI : https://www.doi.org/10.56726/IRJMETS47559 \\nABSTRACT \\nThis technical paper embarks on a comprehensive exploration of the foundational pillars in computer science, \\nnamely Data Structures and Algorithms (DSA), Object -Oriented Programming (OOP), Database Management \\nSystems (DBMS), and Operating Systems (OS). Del ving beyond theoretical boundaries, our journey traverses \\nthe practical landscapes where these domains intersect, illustrating their collective impact on the intricate \\ntapestry of modern computing. From the intricate choreography of data manipulation in DS A to the elegant \\nmodularity of OOP, the strategic data orchestration in DBMS, and the orchestration of hardware resources in \\nOS, this paper navigates the intricate interplay that forms the backbone of technological innovation. By \\nunraveling these key subje cts, this work endeavors to equip both novice learners and seasoned professionals \\nwith a nuanced understanding of the bedrock principles that propel the field of computer science forward into \\nan era of unprecedented possibilities.  \\nKeywords: Data Structure s And Algorithms, Object-Oriented Programming , Database Management Systems , \\nAnd Operating Systems. \\nI. INTRODUCTION \\nIn the dynamic and ever -evolving landscape of computer science, the quest for knowledge transcends the \\nboundaries of theoretical abstraction. This technical paper embarks on a grand intellectual journey, venturing \\ninto the core tenets that underpin the very fabric of modern computing. Data Structures and  \\nAlgorithms (DSA), Object -Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) stand as the cornerstones of this exploration, not merely as disparate subjects but as \\ninterconnected realms that collectively propel the domain of computer science into new frontiers.  \\nThe accelerated pace of technological advancement renders a profound understanding of these fundamental \\nsubjects not just advantageous, but essential. As the a rchitects of the digital age, computer scientists must \\nnavigate the intricate interplay between DSA, OOP, DBMS, and OS to craft solutions that transcend mere \\nfunctionality, embracing elegance and efficiency. It is within this nexus that the boundaries betw een theory and \\npractice blur, and the essence of true innovation is realized.  \\nOur journey into the heart of each domain is not a mere academic exercise but an immersion into the practical \\nrealms where lines of code breathe life into algorithms, where data transforms into actionable insights, and \\nwhere operating systems choreograph the symphony of hardware resources. From themeticulous \\nchoreography of algorithms in DSA to the elegant encapsulation of OOP principles, the strategic orchestration of \\ndatabases in DBMS, and the intricate management of resources in OS, our exploration seeks to bridge the gap \\nbetween foundational principles and real -world applications. This endeavor is more than a mere academic \\npursuit—it is an invitation to comprehend th e language of technology, to decipher the code that powers \\ninnovation, and to grasp the architecture that sustains the digital age. Through this exploration, both novice \\nlearners and seasoned professionals are invited to navigate the intricate crossroads o f DSA, OOP, DBMS, and OS, \\nemerging not just with knowledge but with a holistic comprehension of the structures that underlie every byte \\nof our digital existence. As we embark on this intellectual odyssey, the goal is clear: to empower minds with the \\nwisdom required to not just navigate the currents of contemporary computing but to shape its course and \\ncontribute to the ever-expanding frontier of technological possibilities. In the next sections, we will unravel the \\nintricacies of DSA, OOP, DBMS, and OS, for ging a profound connection between theory and practice in the realm \\nof computer science.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2348] \\nII. FUNDAMENTALS OVERVIEW \\nDSA \\nData Structures and Algorithms (DSA) refer to the foundational concepts in computer science for organizing \\nand processing data efficiently. DSA involves the study of structures like arrays, linked lists, stacks, queues, \\ntrees, and graphs, each serving specific data storage and retrieval purposes. Algorithms are step -by-step \\nprocedures or sets of rules designed to solve computational problems. DSA is fundamental for optimizing data \\nmanipulation and solving complex computational tasks. It is applied in various domains, from software \\ndevelopment to artificial intelligence. DSA enables the creation of efficient search and sorting mechanisms, \\ncontributing to the design of high -performance software systems. Mastery of DSA is crucial for algorithmic \\nproblem-solving and optimizing resource utilization. It provides a framework for understanding the trade -offs \\nbetween different data structures and algorit hms in solving computational challenges. DSA is an integral part of \\ncomputer science education, empowering students and professionals to build scalable and effective software \\nsolutions.  \\nLet's delve into the essence of DSA, unraveling the intricacies of va rious data structures and algorithms that \\nform the backbone of computational thinking. \\n1. Arrays: \\nArrays are fundamental data structures in computer science, representing a contiguous block of memory where \\nelements of the same data type are stored in a linear  fashion. They serve as versatile and efficient containers, \\nproviding a systematic way to organize and access data through a numerical index. The defining characteristic \\nof arrays is their fixed size, established during declaration, making them well -suited for scenarios where the \\nquantity of elements is known in advance. Each element in the array occupies a specific memory location, and \\nits position is determined by its index, starting from zero. This indexing system allows for direct and constant -\\ntime access to elements, enhancing the efficiency of data retrieval. Arrays find extensive application in various \\ndomains, from simple data storage to complex algorithms, offering a foundational building block for the \\nimplementation of more intricate data structure s and computational processes. Despite their fixed size \\nlimitation, arrays' simplicity, speed, and memory efficiency make them indispensable in a multitude of \\nprogramming scenarios.  \\nArrays are widely used in various real -world applications. They play a cr ucial role in image processing for \\nefficient pixel representation, store sensor data in IoT devices, facilitate audio signal processing, organize data \\nin database systems, enable genomic data analysis in bioinformatics, support graphical user interfaces, \\ncontribute to financial modeling, and are essential for mathematical computations using matrices. Arrays \\nprovide a structured and efficient way to organize and access data, making them a foundational data structure \\nin computer science with broad practical implications.  \\nReal-World Application: Student Grading System  \\nIn educational institutions, arrays are commonly used to store and manage student grades efficiently. Imagine a \\nscenario where each student's grades for different subjects are organized using arrays. For instance, an array \\ncould represent the grades for a specific subject, with each element corresponding to a student. The array \\nallows for systematic storage and quick retrieval of grades based on student indices. This structure facilitates \\nstraightforward calculations such as computing averages, identifying highest and lowest scores, and generating \\nreports. The use of arrays in this context streamlines the management of large datasets, providing a scalable \\nand organized solution for tracking and analyzing student performance in diverse academic subjects.  \\n \\nFig. 1: BASIC ARRAY STRUCTURE\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2349] \\n2. Linked Lists: \\nA linked list is a dynamic data structure that consists of a collection of nodes, each containing data and a \\nreference or link to the next node in the sequence. Unlike arrays, linked lists do not require contiguous memory \\nallocation, allowing for flexible and efficient insertion and deletion operations. The first node, known as the \\nhead, serves as the starting point, and the last node typically points to null, indicating the end of the list. Linked \\nlists come in various forms, including singly linked lists where nodes have a reference to the next node, and \\ndoubly linked lists where nodes have references to both the next and previous nodes. This structure allows for \\nsequential traversal, insertion, and removal of elements, making linked lists well -suited for dynamic scenarios \\nwhere the size of the data set may change frequently. While random access to elements is slower compared to \\narrays, linked lists excel in scenarios where dynamic memory allocation and efficient insertion or deletion \\noperations are crucial, such as in applications involving real-time data updates or task scheduling.  \\nLinked lists find practical application in scenarios that require dynamic data management and frequent \\ninsertions or deletions. One notable real -world application is in task sc heduling and management systems. \\nLinked lists enable the efficient representation of tasks, where each node corresponds to a specific task, and the \\nlinks between nodes facilitate sequential processing. As tasks are added or completed, the linked list can b e \\ndynamically adjusted, ensuring a flexible and responsive task scheduling mechanism. This adaptability makes \\nlinked lists valuable in real -time environments, such as operating systems or project management tools, where \\ntask priorities and order may change  dynamically, requiring a data structure that can easily accommodate such \\nupdates.  \\nReal-World Application: Music Playlist  \\nIn the design of music playlist applications, linked lists can be employed to create dynamic and flexible playlists. \\nEach song in th e playlist can be represented as a node in the linked list, where each node contains information \\nabout the song (such as title, artist, and duration) and a reference to the next song in the playlist. This linked list \\nstructure enables easy rearrangement of  songs, insertion of new songs, and removal of existing ones. When a \\nuser adds a new song to the playlist, it becomes a new node linked to the previous song, creating a seamless \\nflow. Similarly, removing a song or rearranging the order involves updating th e references between nodes. This \\ndynamic and adaptable structure makes linked lists an ideal choice for managing playlists, offering a user -\\nfriendly and efficient way to organize and enjoy music in applications like music streaming services.  \\n \\nFig. 2: BASIC STRUCTURE OF LINKED LIST \\n3. Stacks: \\nA stack is a fundamental data structure that follows the Last In, First Out (LIFO) principle, designed to manage a \\ncollection of elements with two primary operations: push, which adds an element to the top of the stack, and \\npop, which removes the topmost element. The stack operates as a dynamic, ordered set where elements are \\nstacked on one another, resembling a vertical structure. The top of the stack is the most recently added \\nelement, while the bottom represents the in itial element. Stacks find widespread application in various \\ncomputing scenarios, such as managing function calls during program execution, undo mechanisms in software \\napplications, and parsing expressions in compilers. The LIFO structure allows for effici ent memory \\nmanagement as elements are added and removed from the top, and the simplicity of these operations makes \\nstacks essential for maintaining order in algorithms, managing recursive processes, and ensuring organized \\ndata storage and retrieval.  \\nStacks are applied in various real -world scenarios due to their Last In, First Out (LIFO) structure. One notable \\napplication is in undo mechanisms of software applications. When users perform actions like typing or \\nformatting, each action is pushed onto a stack . The most recent action is always at the top. If users decide to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2350] \\nundo an operation, the system pops the top element off the stack, effectively reversing the last action. This \\nimplementation provides a straightforward and efficient way to manage a history of user actions, offering a \\nseamless and intuitive undo functionality in applications ranging from text editors to graphic design software.  \\nReal-World Application: Web Browser Navigation  \\nWhen you navigate through web pages using a browser, the stack data  structure is employed to manage the \\nhistory of visited pages and enable the \"Back\" and \"Forward\" functionalities. Each time you visit a new page, it \\nis pushed onto the stack. If you click the \"Back\" button, the browser pops the top page from the stack, \\neffectively taking you back to the previously visited page. Conversely, if you click the \"Forward\" button, the \\nbrowser pushes the next page onto the stack, allowing you to move forward through your browsing history. \\nThis stack-based approach provides a simpl e and efficient way to track and navigate through the sequence of \\nweb pages you have visited, enhancing the overall user experience in web browsing applications.  \\n \\nFig. 3: BASIC STRUCTURE OF STACK \\n4. Queues: \\nA queue is a fundamental data structure that adheres to the First In, First Out (FIFO) principle, serving as an \\nordered collection of elements where insertion occurs at the rear, and removal takes place at the front. This \\ndynamic structure operates like a real-world queue, where entities join at the back and are served or processed \\nfrom the front. Envisioned as a linear arrangement, each element in the queue, often referred to as a \"node,\" \\ncontains data and a reference to the next node in the sequence. Queu es find extensive application in scenarios \\nwhere tasks or data must be processed in the order they are received, such as print job scheduling in operating \\nsystems, managing tasks in asynchronous systems, or modeling processes in computer networks. The \\nsimplicity and efficiency of the FIFO mechanism make queues instrumental in scenarios demanding orderly and \\nsequential data processing, ensuring that the first element enqueued is the first to be dequeued, maintaining a \\nstructured flow of operations. Queues ar e widely employed in scenarios requiring orderly and sequential data \\nprocessing. One prominent real -world application is in print job scheduling within operating systems. Print \\njobs are enqueued as they are submitted to the printer queue, and they are proc essed in the order they are \\nreceived. This First In, First Out (FIFO) approach ensures fairness in task execution, preventing resource \\ncontention and providing an organized mechanism for handling print requests. Queues are integral in managing \\ntasks in a sequential manner, making them valuable in various systems where tasks need to be processed in the \\norder they arrive, contributing to efficient and systematic data flow.  \\nReal-World Application: Customer Support Chat Queue  \\nIn online customer support system s, a queue is often employed to manage incoming customer queries in a fair \\nand organized manner. When customers initiate chat sessions for assistance, their requests join a queue, \\nforming a line based on the order of arrival. The customer service represent atives address inquiries in a First \\nIn, First Out (FIFO) fashion. This ensures that the earliest customer requests are attended to first, maintaining a \\nsense of fairness and timely responsiveness. The queue structure allows for a systematic approach to han dling \\ncustomer inquiries, preventing bottlenecks, and providing an efficient and orderly way to manage the flow of \\ncustomer support interactions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2351] \\n \\nFig. 4: BASIC STRUCTURE OF QUEUE \\n5. Trees: \\nA tree is a hierarchical and widely used data structure in computer  science that represents a collection of \\nelements organized in a branching structure. Comprising nodes connected by edges, a tree consists of a root \\nnode serving as the topmost element, and each node can have zero or more child nodes. Nodes in a tree are \\ninterconnected in a way that no circular paths exist, defining a directed acyclic graph. Nodes that share a \\ncommon parent are considered siblings, and those stemming from the same parent are referred to as subtrees. \\nTrees are characterized by their versatil ity and efficiency in representing hierarchical relationships, making \\nthem fundamental in various domains. They find applications in file systems where directories and files are \\norganized, database indexing for efficient data retrieval, search algorithms l ike binary search, and hierarchical \\ndata representation. With types such as binary trees, AVL trees, and Btrees, the hierarchical structure of trees \\nserves as a powerful paradigm for organizing and navigating complex relationships in diverse computational \\nscenarios.  \\nTrees are applied in various real -world scenarios due to their hierarchical structure. One notable application is \\nin file systems, where trees represent directories and files. The root node signifies the main directory, with \\nbranches extending to subdirectories and leaves representing individual files. This hierarchical organization \\nfacilitates efficient storage, retrieval, and navigation of files, exemplifying how trees enhance the structuring of \\ncomplex relationships. Trees are also pivotal in  database indexing, optimizing search algorithms, and \\nrepresenting hierarchical relationships in applications such as organizational charts and network routing. Their \\nversatility and efficiency make trees a foundational data structure for managing and stru cturing information in \\ndiverse computational environments.  \\nReal-World Application: Company Organizational Chart  \\nConsider a large company with multiple departments, teams, and hierarchical levels. The organizational \\nstructure of such a company can be effe ctively represented using a tree. The root node would symbolize the \\nCEO or the top management, and each subsequent level would represent different management tiers, \\ndepartments, teams, and individual employees. Nodes branching from a higher -level node repr esent the \\nreporting structure, where employees report to their immediate superiors. This tree structure not only mirrors \\nthe organizational hierarchy but also simplifies tasks such as finding reporting relationships, understanding \\ndepartmental structures, and facilitating efficient communication within the company. The use of a tree in this \\ncontext provides a clear and visual representation of the company's organizational framework, aiding in \\ndecision-making, communication, and overall management. \\n \\nFig. 5: BASIC STRUCTURE OF TREE \\n6. Graphs: \\nA graph is a versatile and fundamental data structure in computer science, consisting of a set of nodes (vertices) \\ninterconnected by edges. These edges represent relationships or connections between nodes, and they can be\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2352] \\neither directed or undirected. Graphs can take various forms, including directed graphs (digraphs), where \\nedges have a specific direction, and undirected graphs, where edges have no direction.  \\nNodes in a graph may also have weights or labels, adding additional information to the relationships.  \\nGraphs find broad applications in modeling complex relationships and networks, ranging from social networks \\nand transportation systems to computer networks and project management. Graph algorithms, such as \\nDijkstra's algorithm and breadth -first search, are essential tools for analyzing and solving problems in areas \\nlike route optimization, network flow, and recommendation systems. The flexibility and adaptability of graphs \\nmake them a powerful representation for capt uring and analyzing intricate relationships in diverse \\ncomputational domains.  \\nGraphs are widely employed in various real -world applications due to their ability to model complex \\nrelationships. One notable application is in social networks, where individua ls are represented as nodes, and \\nconnections between them as edges. Graphs enable the analysis of social interactions, identification of \\ninfluential nodes, and the prediction of network behavior. They also play a crucial role in logistics and \\ntransportation systems, where nodes represent locations and edges denote routes. Graph algorithms are \\nutilized for optimizing transportation routes, managing network flow, and enhancing efficiency in diverse \\nscenarios, from package delivery to urban planning. The adapt ability of graphs makes them a versatile tool for \\ncapturing and understanding intricate relationships in dynamic systems.  \\nReal-World Application: Social Network Analysis  \\nConsider a social network, such as Facebook or LinkedIn, where individuals are repre sented as nodes and \\nrelationships between them as edges. This network can be modeled as a graph, with each person being a node, \\nand connections (friendships or professional relationships) forming the edges. Graph algorithms can then be \\napplied to analyze t he structure of the social network, identify key influencers, predict connections, and \\nunderstand the overall connectivity patterns. Social network analysis using graphs has applications in various \\nfields, from targeted advertising and content recommendati on to understanding the spread of information and \\ninfluence within online communities. The graph representation provides a powerful tool for gaining insights \\ninto the dynamics and relationships within complex social systems.  \\n \\nFig. 6: BASIC STRUCTURE OF GRAPH \\n7. Hash Tables: \\nA hashtable, or hash map, is a fundamental data structure that facilitates efficient data retrieval by associating \\nkeys with corresponding values through a process called hashing. It employs a hash function to convert keys \\ninto indices, where the associated values are stored in an array -like structure called a bucket. The key feature of \\na hashtable is its ability to provide constant -time average -case complexity for common operations, such as \\ninsertion, retrieval, and deletion, by minimiz ing collisions —situations where multiple keys hash to the same \\nindex. Collision resolution methods, such as chaining or open addressing, ensure that each bucket can store \\nmultiple key-value pairs. Hashtables find wide application in diverse computing scena rios, including database \\nindexing, symbol tables in compilers, and spell checkers, where the efficient retrieval of data based on keys is \\ncrucial for optimizing algorithmic performance. The adaptability and efficiency of hashtables make them \\nintegral in addressing challenges related to data access and retrieval in various computational domains.  \\nHashtables are extensively applied in various real -world scenarios, notably in database systems for efficient \\ndata retrieval. In this context, keys, representing un ique identifiers or attributes, are hashed using a hash \\nfunction, and the resulting indices point to corresponding values stored in the hashtable. This ensures swift \\naccess to specific data entries, significantly reducing retrieval time compared to linear search methods. \\nHashtables play a pivotal role in optimizing the performance of database systems, making them indispensable\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2353] \\nfor tasks such as quick data lookup, indexing, and retrieval in applications ranging from search engines to \\nfinancial systems, where speed and efficiency in accessing and managing data are critical.  \\nReal-World Application: Spell Checking in Text Editors  \\nHashtables are commonly employed in spell checkers within text editors to ensure efficient and rapid word \\nlookup. In this application, each word in a dictionary is associated with a unique key using a hash function. The \\nhashtable then stores these keys along with their corresponding words. During spell checking, when a user \\ninputs a word, the spell checker hashes the word to find its corresponding key and quickly checks if it exists in \\nthe hashtable. This process allows for near -instantaneous verification of the word's correctness. Hashtables are \\npivotal in this context because they provide constant -time averagecase complexity  for key -based operations, \\nensuring a swift and responsive spell -checking process, which is crucial for enhancing the user experience in \\ntext editing applications. \\n8. Heaps: \\nA heap is a specialized tree -based data structure that satisfies the heap property, which distinguishes it as \\neither a max -heap or a min -heap. In a max -heap, for any given node, the value of the node is greater than or \\nequal to the values of its children, ensuring that the maximum element is at the root. Conversely, in a min -heap, \\nthe value of each node is less than or equal to the values of its children, making the minimum element the root. \\nHeaps are typically implemented as binary trees, where the relationship between parent and child nodes is \\nmaintained by the heap property. The key fea ture of heaps lies in their ability to provide efficient access to the \\nmaximum or minimum element, making them valuable in priority queue implementations. Heaps find \\napplications in various algorithms, including heap sort, Dijkstra's algorithm for shortest  paths, and in -memory \\nsorting operations, where the logarithmic height of the heap ensures fast retrieval and manipulation of extreme \\nvalues. The versatility and performance characteristics of heaps make them a fundamental data structure in \\nalgorithmic design and optimization.  \\nOne notable application of heaps is in priority queues, where elements are assigned priorities, and the highest \\n(or lowest) priority can be efficiently accessed and removed. This is crucial in scenarios such as task scheduling, \\nemergency room triage, and network routing, where tasks, patients, or data packets need to be processed based \\non priority levels. Heaps enable quick identification of the highest -priority element, ensuring that critical \\noperations are performed first. This vers atility makes heaps integral in optimizing resource allocation and task \\nexecution in various real-world applications where prioritization is a key factor.  \\nReal-World Application: Emergency Room Triage  \\nConsider an emergency room (ER) scenario where patien ts arrive with varying degrees of medical urgency. A \\nheap-based priority queue can be employed to manage the triage process efficiently. Each patient is assigned a \\npriority based on the severity of their condition, with the highest priority given to the mo st critical cases. As \\nnew patients arrive, their information, including the severity of their condition, is added to the priority queue \\nimplemented as a max -heap. The patient with the highest priority (most critical condition) is quickly identified \\nand att ended to by the medical staff. This ensures that critical cases are addressed promptly, reflecting the \\nurgency and prioritization inherent in healthcare settings.  \\nThe heap structure allows for constant -time retrieval of the patient with the highest priori ty, facilitating a \\nstreamlined triage process and optimizing the allocation of medical resources in emergency situations.  \\n                 \\n                        Fig. 7: BASIC STRUCTURE OF MAX HEAP          Fig. 8: BASIC STRUCTURE OF MIN HEAP\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2354] \\nOOP \\nObject-Oriented Programming (OOP) is a programming paradigm that structures code around the concept of \\nobjects, which encapsulate data and behavior. OOP principles include encapsulation, where data and methods \\nare bundled within objects, enhancing code mod ularity. Inheritance allows the creation of new classes by \\ninheriting properties and methods from existing ones, promoting code reuse. Polymorphism enhances \\nflexibility by enabling objects of diverse types to be treated as instances of a shared type.  Abstraction allows the \\nrepresentation of complex systems by focusing on essential properties and behaviors. OOP fosters code \\norganization, making it more intuitive and maintainable. Objects communicate through defined interfaces, \\nenhancing modularity and reduci ng dependencies. OOP is widely used in software development for modeling \\nreal-world entities and designing scalable, reusable, and modular code. Java, C++, and Python are popular OOP \\nlanguages. OOP promotes the design of software systems that reflect real -world structures and relationships. \\nHere\\'s an overview of these concepts and other aspects of Object-Oriented Programming:  \\n1. Objects and Classes:  \\nClass: A blueprint or template that defines the properties and behaviors common to all objects of a certai n type.  \\nObject: An instance of a class, representing a concrete realization of the class blueprint.  \\n2. Encapsulation:  \\nEncapsulation involves bundling the data (attributes) and methods (functions) that operate on the data within \\na single unit, known as a class.  \\nThis shields the internal implementation details from the outside world, allowing the object to control access to \\nits data and methods.  \\n3. Inheritance:  \\nInheritance is a mechanism that allows a new class (subclass or derived class) to inherit attri butes and \\nbehaviors from an existing class (superclass or base class).  \\nIt promotes code reusability and establishes an \"is-a\" relationship between the subclasses and the superclass.  \\n4. Polymorphism:  \\nPolymorphism permits the treatment of objects from vari ous classes as instances of a common base class . It \\nincludes method overloading (multiple methods with the same name but different parameters) and method \\noverriding (providing a specific implementation for a method in a subclass).  \\n5. Abstraction:  \\nAbstraction simplifies intricate systems by modeling classes according to the essential properties and \\nbehaviors pertinent to the specific problem.  It helps in managing software complexity by focusing on high -level \\nconcepts.  \\n6. Modularity:  \\nOOP promotes modularity by breaking down a system into smaller, independent, and interchangeable modules \\n(classes). Each module can be developed, tested, and maintained separately, contributing to a more organized \\nand scalable codebase.  \\n7. Encapsulation, Inheritance, and Polymorphism (EIP):  \\nTogether, encapsulation, inheritance, and polymorphism form the core principles of OOP, commonly known as \\nEIP. EIP provides a foundation for building flexible, maintainable, and extensible software systems.  \\n8. Class Relationships:  \\nAssociations: Connections between classes, such as one-to-one, one-to-many, and many to many relationships.  \\nAggregation: A type of association where one class represents a \"whole\" and another class represents a \"part.\"  \\nComposition: A stronger form of aggregation where the \"part\" is tightly bound to the \"whole.\"  \\n9. Design Patterns:  \\nDesign patterns are repetitive solutions addressing prevalent challenges in software design.  OOP supports the \\nimplementation of design patterns, such as Singleton, Factory, Obs erver, and MVC (Model -View-Controller), to \\nimprove code organization and maintainability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2355] \\nOOP is widely used in software development due to its ability to model real -world entities, enhance code \\norganization, and promote code reuse. Popular OOP languages  include Java, C++, Python, and C#.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Banking System:  \\nIn an online banking system, Object -Oriented Programming is commonly employed to model entities such as \\naccounts, transactions, and c ustomers. Each of these entities can be represented as objects with specific \\nproperties (account balance, transaction history) and behaviors (transfer funds, check balance). Inheritance can \\nbe used to represent different types of accounts, while encapsulat ion ensures that sensitive data is hidden and \\nonly accessible through defined methods.  \\n2. E-commerce Platform:  \\nE-commerce platforms utilize OOP to model products, orders, customers, and the shopping cart. Each product \\ncan be an object with properties like  price and description. The shopping cart can be implemented as an object \\nthat manages the addition and removal of products. Inheritance can be applied to represent different types of \\nproducts or discounts, and polymorphism allows for a flexible checkout p rocess.  \\n3. Video Game Development:  \\nIn the gaming industry, OOP is extensively used to model game entities like characters, enemies, weapons, and \\nenvironments. Each game object is represented as a class with specific attributes and behaviors. Inheritance \\ncan be employed to create variations of characters or enemies, and polymorphism allows for dynamic \\ninteractions between different game objects.  \\n4. Hospital Information System:  \\nHospital information systems often use OOP to model patients, doctors, appointme nts, and medical records. \\nEach entity can be represented as an object with relevant attributes and methods. Inheritance can be employed \\nto represent different types of medical professionals, and polymorphism can facilitate the integration of various \\nspecialized modules within the system.  \\n5. Social Media Platform:  \\n \\nFig. 9: ARCHITECTURE OF OOP \\nSocial media platforms extensively leverage OOP to model users, posts, comments, and interactions. Each user \\ncan be represented as an object with properties like username and profile information. Posts and comments \\ncan also be modeled as objects with associated behaviors. Inheritance can be applied to represent different'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2356] \\ntypes of users (regular users, administrators), and polymorphism allows for the dynamic handling of various \\ncontent types. These examples illustrate how Object -Oriented Programming is applied in diverse real -world \\nscenarios to model and manage complex systems by organizing code into modular and reusable structures. \\nOOP provides a scalable and maintainable approach to software develo pment, enabling the creation of systems \\nthat mimic real-world entities and interactions. \\nDBMS \\nA Database Management System (DBMS) is a software tool designed to aid in the generation, administration, \\nand modification of databases. DBMS serves as an interface between the user and the database, providing tools \\nfor data definition, storage, retrieval, and manipulation. It ensures data integrity through features like \\nconstraints, enforcing rules on data stored in databases. D BMS supports the implementation of complex queries \\nand transactions, allowing users to interact with databases efficiently. It provides mechanisms for data security, \\nincluding user authentication, authorization, and encryption. DBMS allows for concurrent a ccess by multiple \\nusers while maintaining consistency and isolation of data. Common types of DBMS include relational, NoSQL, \\nand object-oriented databases. SQL (Structured Query Language) is often used to interact with relational DBMS. \\nDBMS plays a crucial  role in various applications, from business systems to web development, by providing a \\nstructured and efficient means of managing and accessing data.  \\nHere are key aspects of Database Management Systems:  \\n1. Data Definition Language (DDL):  \\nDDL is a subset  of SQL (Structured Query Language) that allows users to define the structure of the database, \\nincluding creating, altering, and deleting tables and establishing relationships between them.  \\n2. Data Manipulation Language (DML):  \\nDML enables users to interac t with the data stored in the database. Common DML operations include inserting, \\nupdating, and deleting records, as well as querying data using SELECT statements.  \\n3. Data Integrity:  \\nDBMS ensures data integrity by enforcing constraints such as primary keys , foreign keys, unique constraints, \\nand check constraints. These restrictions are in place to avoid the storage of data that is either invalid or \\ninconsistent in the database. \\n4. Concurrency Control:  \\nConcurrency control mechanisms in DBMS manage simultaneo us access to data by multiple users to ensure \\ndata consistency. Techniques like locking and transaction isolation levels are employed to prevent conflicts.  \\n5. Transaction Management:  \\nDBMS supports transactions, which are sequences of one or more SQL opera tions treated as a single unit of \\nwork. Transactions ensure data consistency by either committing changes if successful or rolling back if an \\nerror occurs.  \\n6. Data Security:  \\nDBMS provides mechanisms for securing data, including user authentication, author ization, and access control. \\nUsers are granted specific permissions to perform operations on certain data, preventing unauthorized access.  \\n7. Data Modeling:  \\nDBMS enables the creation of data models to represent the structure and relationships within the database. \\nTypical data models encompass the relational model, hierarchical model, network model, and object -oriented \\nmodel. \\n8. Query Optimization:  \\nDBMS optimiz es queries to enhance performance. Query optimization involves selecting the most efficient \\nexecution plan for a given query, considering factors like indexes, join algorithms, and access methods.  \\n9. Normalization:  \\nNormalization refers to the method of st ructuring data to minimize redundancy and enhance data integrity.  \\nThis involves decomposing tables and ensuring that data dependencies are minimized.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2357] \\n10. Backup and Recovery:  \\nDBMS provides tools for creating backups of the database to prevent data loss in  case of hardware failure, \\nsoftware errors, or other disasters. Recovery mechanisms restore the database to a consistent state after a \\nfailure.  \\n11. Scalability:  \\nDBMS systems are designed to scale, allowing for the management of large datasets and handling  a growing \\nnumber of concurrent users. Scalability is crucial for accommodating expanding data requirements.  \\n12. Data Warehousing and Data Mining:  \\nDBMS is often integrated with data warehousing and data mining tools to support the extraction, \\ntransformation, and loading (ETL) of data, as well as the analysis and discovery of patterns and trends in large \\ndatasets.  \\nPopular Database Management Systems  include MySQL, Oracle Database, Microsoft SQL Server, PostgreSQL, \\nand MongoDB. The choice of a DBMS depends on the specific requirements of the application, data model \\npreferences, and scalability needs.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Retail and E-commerce Platforms:  \\nOnline retail platforms, such as Amazon and eBay, heavily rely on DBMS to manage vast product catalogs, \\ncustomer information, order processing, and inventory. The system ensures quick and accurate retrie val of \\nproduct information, tracks customer orders, and supports seamless transactions.  \\n2. Airline Reservation Systems:  \\nAirline reservation systems, like those used by airlines worldwide, utilize DBMS to manage flight schedules, \\nseat availability, passeng er information, and ticketing. This ensures efficient booking processes, tracks \\npassenger details, and allows for dynamic adjustments to flight schedules.  \\n3. Human Resource Management Systems (HRMS):  \\nHRMS applications, used by companies for personnel mana gement, leverage DBMS to store employee records, \\npayroll information, attendance data, and performance evaluations. The system ensures the secure and \\norganized management of HR-related information.  \\n4. Social Media Platforms:  \\nSocial media platforms, includ ing Facebook, Twitter, and Instagram, employ DBMS to manage user profiles, \\nposts, comments, and social connections. The system enables rapid retrieval of personalized content, tracks \\nuser interactions, and supports features like recommendations and targete d advertising.  \\n5. Library Management Systems:  \\nLibrary management systems, used by educational institutions and public libraries, utilize DBMS to organize \\nand catalog books, manage borrower information, and track lending transactions. The system ensures ef ficient \\nlibrary operations, including book searches, checkouts, and returns.  \\nThese examples showcase the diverse applications of DBMS in different industries, highlighting its role in \\norganizing, storing, and retrieving data to support critical business functions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2358] \\n \\nFig. 10: ARCHITECTURE OF DBMS \\nOS \\nAn Operating System (OS) is a software that acts as an intermediary between computer hardware and user \\napplications, managing hardware resources and facilitating seamless execution of software. It provides \\nessential services like process management, handli ng the creation, scheduling, and termination of processes. \\nMemory management allocates and deallocates memory space for processes, ensuring efficient utilization. File \\nsystem management organizes and stores data on storage devices, allowing for file creati on, deletion, and \\nmanipulation. Device management facilitates communication between the OS and hardware devices, such as \\ninput/output peripherals. Security features include user authentication, access controls, and encryption to \\nsafeguard the system. The O S provides a user interface (UI), which can be command -line or graphical, enabling \\nuser interaction. Networking capabilities support communication between computers and devices within a \\nnetwork. Error handling mechanisms address issues and maintain system stability. The OS is fundamental for \\nthe execution of diverse software applications and is integral to the functionality of computers and devices.  \\nHere are key aspects of Operating Systems:  \\n1. Kernel:  \\nThe kernel constitutes the central element of the ope rating system.  It provides essential services, including \\nprocess management, memory management, device management, and system calls. The kernel interacts \\ndirectly with the hardware and ensures that different software components can run efficiently.  \\n2. Process Management:  \\nProcess management encompasses the initiation, scheduling, and cessation of processes.  The OS manages the \\nexecution of multiple processes concurrently, allowing users to run multiple applications simultaneously.  \\n3. Memory Management:  \\nMemory management is responsible for allocating and deallocating memory space for processes. The OS \\nensures efficient utilization of memory, handles memory protection, and facilitates virtual memory to extend \\navailable RAM.  \\n4. File System Management:  \\nThe OS m anages file systems, organizing and storing data on storage devices such as hard drives. It provides \\nfile-related services, including file creation, deletion, reading, and writing. File systems organize data into \\ndirectories and files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2359] \\n5. Device Management:  \\nDevice management handles communication between the OS and hardware devices, including input/output \\ndevices like keyboards, printers, and storage devices. It provides drivers and interfaces to enable \\ncommunication and control of these devices.  \\n6. Security and Protection:  \\nThe operating system enforces security protocols to safeguard both the system and user data.  This includes \\nuser authentication, access controls, encryption, and security patches to address vulnerabilities. The OS also \\nenforces memory protection to prevent unauthorized access to memory areas.  \\n7. User Interface:  \\nThe user interface (UI) provides a means for users to interact with the computer. OS can have a command -line \\ninterface (CLI) or a graphical user interface (GUI). GUIs include elem ents like windows, icons, menus, and \\nbuttons to enhance user experience.  \\n8. Networking:  \\nNetworking capabilities enable computers to communicate with each other in a network. The OS provides \\nnetworking protocols and services, facilitating tasks such as fil e sharing, internet access, and remote access to \\nresources.  \\n9. Schedulers:  \\nSchedulers manage the execution of processes and allocate CPU time. They include process schedulers for \\nshort-term CPU scheduling, and long -term schedulers for selecting processes to be brought into the ready \\nqueue.  \\n10. Error Handling:  \\nOS handles errors and exceptions to maintain system stability. It provides error messages, logs, and recovery \\nmechanisms to address issues that may arise during system operation.  \\n11. System Calls:  \\nSystem calls are interfaces through which user -level programs request services from the operating system. \\nCommon system calls include file operations, process control, and memory management.  \\n12. Boot Process:  \\nThe boot process initializes the computer's har dware and loads the OS into memory. It involves the BIOS/UEFI, \\nbootloader, and kernel, leading to the full initialization of the OS.  \\nOperating systems come in various types, including Windows, macOS, Linux, Unix, and others. Each serve as a \\nfundamental layer in the computing stack, enabling the execution of applications and providing a user -friendly \\nenvironment for users to interact with their devices.  \\nHere are two examples of real-world applications that extensively rely on Operating Systems:  \\n1. Automated Teller Machines (ATMs):  \\nAutomated Teller Machines, commonly found in banks and other financial institutions, heavily depend on \\noperating systems for their functionality. The OS manages interactions between the ATM hardware \\ncomponents, such as the card reader, cash dispenser, and keypad. It handles security features, user \\nauthentication, and communication with the bank's servers. The OS ensures that transactions are processed \\nsecurely and efficiently, providing users with a seamless and reliable  experience.  \\n2. Smartphones:  \\nSmartphones, including those running iOS (Apple), Android, or other mobile operating systems, are ubiquitous \\ndevices that rely on operating systems to manage a wide range of functions. The OS on a smartphone handles \\nthe user interface, application management, memory allocation, power management, and communication with \\nvarious hardware components (e.g., camera, GPS, sensors). It ensures a smooth user experience by coordinating \\nthe execution of diverse applications  and services, making smartphones an integral part of everyday life. These \\nexamples illustrate how operating systems play a crucial role in diverse technological applications, enabling the \\neffective and secure functioning of complex systems.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2360] \\n \\nFig. 11: ARCHITECTURE OF OS \\nIII. METHODOLOGY \\nIn the landscape of computer science, the integration and interconnectivity among Data Structures and \\nAlgorithms (DSA), Object -Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) form the backbone of s ophisticated software development and system design. Each of \\nthese fundamental subjects contributes distinct elements to the overall architecture of a computing system, and \\ntheir seamless integration is essential for creating robust and efficient applicati ons.  \\nHere's an exploration of how these subjects intersect:  \\n1. DSA in System Design:  \\nData Structures and Algorithms play a pivotal role in designing efficient and scalable systems. DSA is the \\nbedrock for optimizing data storage, retrieval, and manipulati on. Algorithms, whether for sorting, searching, or \\noptimizing, are crucial for creating performant applications. In conjunction with DSA, system architects can \\ndesign data models that align with the requirements of both DBMS and OOP, ensuring effective uti lization of \\nresources.  \\n2. OOP for Code Organization and Reusability:  \\nObject-Oriented Programming provides a paradigm for organizing code into modular and reusable \\ncomponents. Objects encapsulate data and behavior, facilitating the creation of software ent ities that model \\nreal-world entities. In an integrated system, OOP principles enable clean interfaces and interactions between \\ndifferent modules. DSA structures can be seamlessly integrated into OOP -designed systems, allowing for \\nefficient data manipulation and algorithm execution within the context of object-oriented applications.  \\n3. DBMS for Persistent Data Storage:  \\nDatabase Management Systems are central to storing and retrieving data persistently. In an integrated system, \\nthe interaction between DSA and DBMS is evident in how data structures are translated into database schemas.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2361] \\nDSA concepts like indexing and normalization directly impact how data is organized in a database. The system\\'s \\narchitecture, shaped by DSA, ensures that data is efficiently managed and manipulated through DBMS, \\nproviding a structured and scalable approach to data storage.  \\n4. OS for Resource Management and Execution:  \\nThe Operating System acts as the underlying platform that orchestrates the execution of software applications. \\nIn an integrated system, the OS interacts with applications designed using OOP principles, allocates memory \\nbased on DS A requirements, and manages access to data stored in DBMS. The OS ensures the efficient \\nutilization of hardware resources, providing a foundation for the execution of diverse software components.  \\nExamples of Interconnectivity:  \\n1. E-commerce Platform:  \\nAn e -commerce platform exemplifies the interconnectivity of DSA, OOP, DBMS, and OS. DSA algorithms are \\nemployed for efficient search and recommendation systems. OOP principles are applied for organizing code \\ninto modular components. DBMS is used to store a nd retrieve product information, user data, and transaction \\nrecords. The OS ensures the secure and efficient execution of the entire system.  \\n2. Healthcare Information System:  \\nIn a healthcare information system, DSA is used for optimizing patient data str uctures, OOP principles guide the \\norganization of healthcare modules, DBMS manages patient records, and the OS ensures the secure and reliable \\noperation of the entire system. The interplay of these fundamental subjects is crucial for maintaining the \\nintegrity and efficiency of healthcare information systems. In essence, the integration of DSA, OOP, DBMS, and \\nOS is not merely a theoretical concept but a practical necessity for building sophisticated, interconnected, and \\nhigh-performance computing systems. Un derstanding how these subjects complement each other is essential \\nfor professionals and students alike, as it lays the groundwork for effective problem -solving and innovation in \\nthe ever-evolving landscape of computer science. \\nIV. CONCLUSION \\nIn conclusion, th is paper underscores the foundational significance of Data Structures and Algorithms (DSA), \\nObject-Oriented Programming (OOP), Database Management Systems (DBMS), and Operating Systems (OS) in \\nthe realm of computer science. The paper emphasizes that a stro ng and comprehensive understanding of these \\nfundamental subjects is not only beneficial but essential for success in the dynamic and ever -evolving field of \\ncomputer science. Whether for students embarking on their educational journey or professionals navig ating the \\nintricacies of software development, the knowledge gained in DSA, OOP, DBMS, and OS serves as a bedrock for \\nbuilding innovative solutions, tackling challenges, and contributing meaningfully to the advancement of \\ntechnology. \\nV. REFERENCES \\n[1] R. K. Vosi lovich, \"Universal Linear Data Structure,\" 2020 International Conference on Information \\nScience and Communications Technologies (ICISCT), Tashkent, Uzbekistan, 2020, pp. 1 -3, doi: \\n10.1109/ICISCT50599.2020.9351485.  \\n[2] Donald Knuth. The Art of Computer Program ming, Volume 1: Fundamental Algorithms, Third Edition. \\nAddison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.2.1: Stacks, Queues, and Deques, pp. 238–243.  \\n[3] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to \\nAlgorithms, Second Edition. MIT Press and McGraw -Hill, 2001. ISBN 0 -262-03293-7. Section 10.1: \\nStacks and queues, pp. 200–204.  \\n[4] E. Vaahedi and K. W. Cheung, \"Evolution and future of on -line DSA,\" 2000 IEEE Power Engineering \\nSociety Winter Meeting. Conference Proce edings (Cat. No.00CH37077), Singapore, 2000, pp. 63 -65 \\nvol.1, doi: 10.1109/PESW.2000.849928.  \\n[5] T. Mudner and E. Shakshuki, \"A new approach to learning algorithms,\" International Conference on \\nInformation Technology: Coding and Computing, 2004. Proceedings. ITCC 2004., Las Vegas, NV, USA, \\n2004, pp. 141-145 Vol.1, doi: 10.1109/ITCC.2004.1286440.  \\n[6] Summer Meeting. Conference Proceedings (Cat. No.01CH37262), Vancouver, BC, Canada, 2001, pp. \\n1070-1074 vol.2, doi: 10.1109/PESS.2001.970207.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2362] \\n[7] B. Stroustrup, \"What is object -oriented programming?,\" in IEEE Software, vol. 5, no. 3, pp. 1020, May \\n1988, doi: 10.1109/52.2020.  \\n[8] G. Schlageter et al., \"OOPS -an object oriented programming system with integrated data management \\nfacility,\" Proceedings. Fourth International Conference on Data Engineering, Los Angeles, CA, USA, \\n1988, pp. 118-125, doi: 10.1109/ICDE.1988.105453. \\n[9] S. Abbasi, H. Kazi and K. Khowaja, \"A systematic review of learning object oriented programming \\nthrough serious games and programming approaches,\" 2017 4th IEEE International  Conference on \\nEngineering Technologies and Applied Sciences (ICETAS), Salmabad, Bahrain, 2017, pp. 1-6,  \\ndoi: 10.1109/ICETAS.2017.8277894. \\n[10] R. J. D\\'Andrea and R. G. Gowda, \"Object -oriented programming: concepts and languages,\" IEEE \\nConference on Aerospace and Electronics, Dayton, OH, USA, 1990, pp. 634-640 vol.2,  \\ndoi: 10.1109/NAECON.1990.112840.  \\n[11] S. Samaiya and M. Agarwal, \"Real time database management system,\" 2018 2nd International \\nConference on Inventive Systems and Control (ICISC), Coimbatore, India, 2018, pp. 903 -908,  \\ndoi: 10.1109/ICISC.2018.8398931.  \\n[12] Wiederhold, \"Databases,\" in Computer, vol. 17, no. 10, pp. 211-223, Oct. 1984,  \\ndoi: 10.1109/MC.1984.1658971.  \\n[13] P. G. Selinger, \"Database technology,\" in IBM Systems Journal, vol. 26, no. 1, pp. 96 -106, 1987,  \\ndoi: 10.1147/sj.261.0096.  \\n[14] A. Corallo, M. Espostito, A . Massafra and S. Totaro, \"A Relational Database Management System \\nApproach for Data Integration in Manufacturing Process,\" 2018 IEEE International Conference on \\nEngineering, Technology and Innovation (ICE/ITMC), Stuttgart, Germany, 2018, pp. 1 -7,  \\ndoi: 10.1109/ICE.2018.8436290.  \\n[15] S. H. Son, \"Real -time database systems: present and future,\" Proceedings Second International \\nWorkshop on Real-Time Computing Systems and Applications, Tokyo, Japan, 1995, pp. 50 -52,  \\ndoi: 10.1109/RTCSA.1995.528750.  \\n[16] R. R. Muntz, \"Operating systems,\" in Computer, vol. 7, no. 6, pp. 21-21, June 1974,  \\ndoi: 10.1109/MC.1974.6323579.  \\n[17] W. Chengjun, \"The Analyses of Operating System Structure,\" 2009 Second International Symposium on \\nKnowledge Acquisition and Modeling, Wuhan, China, 2009, pp. 354-357, doi: 10.1109/KAM.2009.265.  \\n[18] Norman F. Schneidewind, \"Operating Systems,\" in Computer, Network, Software, and Hardware \\nEngineering with Applications , IEEE, 2012, pp.286-302, doi: 10.1002/9781118181287.ch10.  \\n[19] H. Mei and Y. Guo, \"Operating Systems for Internetware: Challenges and Future Directions,\" 2018 IEEE \\n38th International Conference on Distributed Computing Systems (ICDCS), Vienna, Austria, 2018, pp. \\n1377-1384, doi: 10.1109/ICDCS.2018.00138.  \\n[20] C. Peng, X. -q. Yang, Z. -z. Niu and X. -p. Liu, \"R esearch on Windows operating system education,\" 2009 \\nIEEE International Symposium on IT in Medicine & Education, Jinan, China, 2009, pp. 719 -724, doi: \\n10.1109/ITIME.2009.5236327.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3e476",
   "metadata": {},
   "source": [
    "### Text Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf1eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 29 documents into 131 chunks\n",
      "\n",
      "Example chunk: \n",
      "Content: Hello! How can I assist you today?\n",
      "When resetting Windows 11 in a dual-boot system with Linux, the Windows reset process may\n",
      "overwrite the bootloader, which can remove or interfere with the GRUB bootl...\n",
      "Metadata: {'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content='Hello! How can I assist you today?\\nWhen resetting Windows 11 in a dual-boot system with Linux, the Windows reset process may\\noverwrite the bootloader, which can remove or interfere with the GRUB bootloader that manages\\nLinux boot. To avoid losing GRUB when resetting Windows 11, consider these key points:\\nIn summary, resetting Windows 11 alone tends to remove GRUB, but if you are prepared to\\nrestore GRUB using a Linux Live USB afterward, the dual boot can be recovered without data\\nloss on Linux. Carefully manage BIOS boot priorities and be ready to reinstall GRUB if needed.\\nIf preferred, detailed steps for restoring GRUB after a Windows reset or installation are available,\\nincluding mounting the Linux partition and running sudo grub-install commands from a Live\\nenvironment. Would you like step-by-step instructions for restoring GRUB after resetting\\nWindows?\\n⁂\\nHi\\nI have windows 11 and linux I want to reset the\\nwindows and I dont want the grub to be removed'),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content=\"environment. Would you like step-by-step instructions for restoring GRUB after resetting\\nWindows?\\n⁂\\nHi\\nI have windows 11 and linux I want to reset the\\nwindows and I dont want the grub to be removed\\nBefore the reset, ensure you have a Linux Live USB or rescue media ready, so you can\\nrestore GRUB if it's overwritten. Windows reset typically replaces GRUB with the Windows\\nbootloader, removing the Linux boot option.\\nAfter resetting Windows, if GRUB is missing, it can be restored using a Linux Live\\nenvironment by reinstalling GRUB with commands like grub-install and update-grub\\ntargeted at your Linux partition.\\nAlternatively, checking BIOS/UEFI boot order before and after the reset to ensure the Linux\\nbootloader remains prioritized can prevent Windows from automatically booting and\\nbypassing GRUB.\\nDisabling Windows fast startup before the reset reduces potential interference with the\\nbootloader.\\nSome users suggest setting the Linux disk as primary boot device in BIOS/UEFI, so the\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content='Disabling Windows fast startup before the reset reduces potential interference with the\\nbootloader.\\nSome users suggest setting the Linux disk as primary boot device in BIOS/UEFI, so the\\nGRUB loader controls the boot process, even if Windows modifies its own bootloader.\\n\\x001\\x00\\n\\x002\\x00\\x003\\x00\\n\\x004\\x00\\x001\\x00\\n\\x00\\x00\\x00https://pq.hosting/en/help/restoring-grub-after-installing-windows-10\\n\\x00\\x00\\x00https://www.reddit.com/r/linux4noobs/comments/w6qtrx/will_the_windows_11_update_break_dual_boot/'),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content='\\x00\\x00\\x00https://discussion.fedoraproject.org/t/repair-reinstall-grub-after-windows-11-update-dual-boot-fedora\\n-f39/113155\\n\\x00\\x00\\x00https://www.cybrary.it/blog/restoring-grub-installing-windows\\n\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/4166685/(solved)-accidentally-deleted-windows-1\\n1-microsoft?forum=windows-all\\n\\x00\\x00\\x00https://www.reddit.com/r/linuxmint/comments/1fmn07i/reinstalling_windows_wiped_my_grub_but_now_i\\n_cant/\\n\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/3957734/why-isnt-my-windows-booting-after-du\\nal-booting-lin\\n\\x00\\x00\\x00https://forum.manjaro.org/t/remove-grub-and-revert-to-windows-bootloader/103716\\n\\x00\\x00\\x00https://www.youtube.com/watch?v=PVxDttc2z-I\\n\\x00\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/4115492/i-deleted-a-partition-from-windows-11-th\\nat-had-lin\\n\\x00\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/2796971/how-to-restore-windows-bootloader-fro\\nm-grub?forum=windows-all\\n\\x00\\x00\\x00\\x00https://wiki.archlinux.org/title/Dual_boot_with_Windows'),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'moddate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'pplx_search.pdf', 'file_type': 'pdf'}, page_content='at-had-lin\\n\\x00\\x00\\x00\\x00https://learn.microsoft.com/en-us/answers/questions/2796971/how-to-restore-windows-bootloader-fro\\nm-grub?forum=windows-all\\n\\x00\\x00\\x00\\x00https://wiki.archlinux.org/title/Dual_boot_with_Windows\\n\\x00\\x00\\x00\\x00https://discourse.ubuntu.com/t/grub-install-issues-on-laptop-with-win11/52702\\n\\x00\\x00\\x00\\x00https://www.reddit.com/r/linuxmint/comments/1hawohz/how_to_restore_window_11_bootloader_no_longe\\nr/\\n\\x00\\x00\\x00\\x00https://www.youtube.com/watch?v=I_1fk15QQ_M\\n\\x00\\x00\\x00\\x00https://www.youtube.com/watch?v=KWVte9WGxGE\\n\\x00\\x00\\x00\\x00https://forum.endeavouros.com/t/no-grub-after-windows-10-install-how-to-recover/45977\\n\\x00\\x00\\x00\\x00https://www.onlogic.com/blog/how-to-dual-boot-windows-11-and-linux/'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='of 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='single-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2347] \\nEXPLORING TECH FOUNDATIONS: DSA, OOP, DBMS, OS IN  \\nCOMPUTER SCIENCE \\nSushmita C. Hubli*1 \\n*1Department Of Electronics And Telecommunication, Pune Institute Of Computer Technology,  \\nPune, India. \\nDOI : https://www.doi.org/10.56726/IRJMETS47559 \\nABSTRACT \\nThis technical paper embarks on a comprehensive exploration of the foundational pillars in computer science,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Pune, India. \\nDOI : https://www.doi.org/10.56726/IRJMETS47559 \\nABSTRACT \\nThis technical paper embarks on a comprehensive exploration of the foundational pillars in computer science, \\nnamely Data Structures and Algorithms (DSA), Object -Oriented Programming (OOP), Database Management \\nSystems (DBMS), and Operating Systems (OS). Del ving beyond theoretical boundaries, our journey traverses \\nthe practical landscapes where these domains intersect, illustrating their collective impact on the intricate \\ntapestry of modern computing. From the intricate choreography of data manipulation in DS A to the elegant \\nmodularity of OOP, the strategic data orchestration in DBMS, and the orchestration of hardware resources in \\nOS, this paper navigates the intricate interplay that forms the backbone of technological innovation. By \\nunraveling these key subje cts, this work endeavors to equip both novice learners and seasoned professionals'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='unraveling these key subje cts, this work endeavors to equip both novice learners and seasoned professionals \\nwith a nuanced understanding of the bedrock principles that propel the field of computer science forward into \\nan era of unprecedented possibilities.  \\nKeywords: Data Structure s And Algorithms, Object-Oriented Programming , Database Management Systems , \\nAnd Operating Systems. \\nI. INTRODUCTION \\nIn the dynamic and ever -evolving landscape of computer science, the quest for knowledge transcends the \\nboundaries of theoretical abstraction. This technical paper embarks on a grand intellectual journey, venturing \\ninto the core tenets that underpin the very fabric of modern computing. Data Structures and  \\nAlgorithms (DSA), Object -Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) stand as the cornerstones of this exploration, not merely as disparate subjects but as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Operating Systems (OS) stand as the cornerstones of this exploration, not merely as disparate subjects but as \\ninterconnected realms that collectively propel the domain of computer science into new frontiers.  \\nThe accelerated pace of technological advancement renders a profound understanding of these fundamental \\nsubjects not just advantageous, but essential. As the a rchitects of the digital age, computer scientists must \\nnavigate the intricate interplay between DSA, OOP, DBMS, and OS to craft solutions that transcend mere \\nfunctionality, embracing elegance and efficiency. It is within this nexus that the boundaries betw een theory and \\npractice blur, and the essence of true innovation is realized.  \\nOur journey into the heart of each domain is not a mere academic exercise but an immersion into the practical \\nrealms where lines of code breathe life into algorithms, where data transforms into actionable insights, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='realms where lines of code breathe life into algorithms, where data transforms into actionable insights, and \\nwhere operating systems choreograph the symphony of hardware resources. From themeticulous \\nchoreography of algorithms in DSA to the elegant encapsulation of OOP principles, the strategic orchestration of \\ndatabases in DBMS, and the intricate management of resources in OS, our exploration seeks to bridge the gap \\nbetween foundational principles and real -world applications. This endeavor is more than a mere academic \\npursuit—it is an invitation to comprehend th e language of technology, to decipher the code that powers \\ninnovation, and to grasp the architecture that sustains the digital age. Through this exploration, both novice \\nlearners and seasoned professionals are invited to navigate the intricate crossroads o f DSA, OOP, DBMS, and OS, \\nemerging not just with knowledge but with a holistic comprehension of the structures that underlie every byte'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='emerging not just with knowledge but with a holistic comprehension of the structures that underlie every byte \\nof our digital existence. As we embark on this intellectual odyssey, the goal is clear: to empower minds with the \\nwisdom required to not just navigate the currents of contemporary computing but to shape its course and \\ncontribute to the ever-expanding frontier of technological possibilities. In the next sections, we will unravel the \\nintricacies of DSA, OOP, DBMS, and OS, for ging a profound connection between theory and practice in the realm \\nof computer science.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2348] \\nII. FUNDAMENTALS OVERVIEW \\nDSA \\nData Structures and Algorithms (DSA) refer to the foundational concepts in computer science for organizing \\nand processing data efficiently. DSA involves the study of structures like arrays, linked lists, stacks, queues, \\ntrees, and graphs, each serving specific data storage and retrieval purposes. Algorithms are step -by-step'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='trees, and graphs, each serving specific data storage and retrieval purposes. Algorithms are step -by-step \\nprocedures or sets of rules designed to solve computational problems. DSA is fundamental for optimizing data \\nmanipulation and solving complex computational tasks. It is applied in various domains, from software \\ndevelopment to artificial intelligence. DSA enables the creation of efficient search and sorting mechanisms, \\ncontributing to the design of high -performance software systems. Mastery of DSA is crucial for algorithmic \\nproblem-solving and optimizing resource utilization. It provides a framework for understanding the trade -offs \\nbetween different data structures and algorit hms in solving computational challenges. DSA is an integral part of \\ncomputer science education, empowering students and professionals to build scalable and effective software \\nsolutions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"computer science education, empowering students and professionals to build scalable and effective software \\nsolutions.  \\nLet's delve into the essence of DSA, unraveling the intricacies of va rious data structures and algorithms that \\nform the backbone of computational thinking. \\n1. Arrays: \\nArrays are fundamental data structures in computer science, representing a contiguous block of memory where \\nelements of the same data type are stored in a linear  fashion. They serve as versatile and efficient containers, \\nproviding a systematic way to organize and access data through a numerical index. The defining characteristic \\nof arrays is their fixed size, established during declaration, making them well -suited for scenarios where the \\nquantity of elements is known in advance. Each element in the array occupies a specific memory location, and \\nits position is determined by its index, starting from zero. This indexing system allows for direct and constant -\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"its position is determined by its index, starting from zero. This indexing system allows for direct and constant -\\ntime access to elements, enhancing the efficiency of data retrieval. Arrays find extensive application in various \\ndomains, from simple data storage to complex algorithms, offering a foundational building block for the \\nimplementation of more intricate data structure s and computational processes. Despite their fixed size \\nlimitation, arrays' simplicity, speed, and memory efficiency make them indispensable in a multitude of \\nprogramming scenarios.  \\nArrays are widely used in various real -world applications. They play a cr ucial role in image processing for \\nefficient pixel representation, store sensor data in IoT devices, facilitate audio signal processing, organize data \\nin database systems, enable genomic data analysis in bioinformatics, support graphical user interfaces,\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"in database systems, enable genomic data analysis in bioinformatics, support graphical user interfaces, \\ncontribute to financial modeling, and are essential for mathematical computations using matrices. Arrays \\nprovide a structured and efficient way to organize and access data, making them a foundational data structure \\nin computer science with broad practical implications.  \\nReal-World Application: Student Grading System  \\nIn educational institutions, arrays are commonly used to store and manage student grades efficiently. Imagine a \\nscenario where each student's grades for different subjects are organized using arrays. For instance, an array \\ncould represent the grades for a specific subject, with each element corresponding to a student. The array \\nallows for systematic storage and quick retrieval of grades based on student indices. This structure facilitates \\nstraightforward calculations such as computing averages, identifying highest and lowest scores, and generating\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='straightforward calculations such as computing averages, identifying highest and lowest scores, and generating \\nreports. The use of arrays in this context streamlines the management of large datasets, providing a scalable \\nand organized solution for tracking and analyzing student performance in diverse academic subjects.  \\n \\nFig. 1: BASIC ARRAY STRUCTURE'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2349] \\n2. Linked Lists: \\nA linked list is a dynamic data structure that consists of a collection of nodes, each containing data and a \\nreference or link to the next node in the sequence. Unlike arrays, linked lists do not require contiguous memory \\nallocation, allowing for flexible and efficient insertion and deletion operations. The first node, known as the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='allocation, allowing for flexible and efficient insertion and deletion operations. The first node, known as the \\nhead, serves as the starting point, and the last node typically points to null, indicating the end of the list. Linked \\nlists come in various forms, including singly linked lists where nodes have a reference to the next node, and \\ndoubly linked lists where nodes have references to both the next and previous nodes. This structure allows for \\nsequential traversal, insertion, and removal of elements, making linked lists well -suited for dynamic scenarios \\nwhere the size of the data set may change frequently. While random access to elements is slower compared to \\narrays, linked lists excel in scenarios where dynamic memory allocation and efficient insertion or deletion \\noperations are crucial, such as in applications involving real-time data updates or task scheduling.  \\nLinked lists find practical application in scenarios that require dynamic data management and frequent'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Linked lists find practical application in scenarios that require dynamic data management and frequent \\ninsertions or deletions. One notable real -world application is in task sc heduling and management systems. \\nLinked lists enable the efficient representation of tasks, where each node corresponds to a specific task, and the \\nlinks between nodes facilitate sequential processing. As tasks are added or completed, the linked list can b e \\ndynamically adjusted, ensuring a flexible and responsive task scheduling mechanism. This adaptability makes \\nlinked lists valuable in real -time environments, such as operating systems or project management tools, where \\ntask priorities and order may change  dynamically, requiring a data structure that can easily accommodate such \\nupdates.  \\nReal-World Application: Music Playlist  \\nIn the design of music playlist applications, linked lists can be employed to create dynamic and flexible playlists.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='updates.  \\nReal-World Application: Music Playlist  \\nIn the design of music playlist applications, linked lists can be employed to create dynamic and flexible playlists. \\nEach song in th e playlist can be represented as a node in the linked list, where each node contains information \\nabout the song (such as title, artist, and duration) and a reference to the next song in the playlist. This linked list \\nstructure enables easy rearrangement of  songs, insertion of new songs, and removal of existing ones. When a \\nuser adds a new song to the playlist, it becomes a new node linked to the previous song, creating a seamless \\nflow. Similarly, removing a song or rearranging the order involves updating th e references between nodes. This \\ndynamic and adaptable structure makes linked lists an ideal choice for managing playlists, offering a user -\\nfriendly and efficient way to organize and enjoy music in applications like music streaming services.  \\n \\nFig. 2: BASIC STRUCTURE OF LINKED LIST'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='friendly and efficient way to organize and enjoy music in applications like music streaming services.  \\n \\nFig. 2: BASIC STRUCTURE OF LINKED LIST \\n3. Stacks: \\nA stack is a fundamental data structure that follows the Last In, First Out (LIFO) principle, designed to manage a \\ncollection of elements with two primary operations: push, which adds an element to the top of the stack, and \\npop, which removes the topmost element. The stack operates as a dynamic, ordered set where elements are \\nstacked on one another, resembling a vertical structure. The top of the stack is the most recently added \\nelement, while the bottom represents the in itial element. Stacks find widespread application in various \\ncomputing scenarios, such as managing function calls during program execution, undo mechanisms in software \\napplications, and parsing expressions in compilers. The LIFO structure allows for effici ent memory'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='applications, and parsing expressions in compilers. The LIFO structure allows for effici ent memory \\nmanagement as elements are added and removed from the top, and the simplicity of these operations makes \\nstacks essential for maintaining order in algorithms, managing recursive processes, and ensuring organized \\ndata storage and retrieval.  \\nStacks are applied in various real -world scenarios due to their Last In, First Out (LIFO) structure. One notable \\napplication is in undo mechanisms of software applications. When users perform actions like typing or \\nformatting, each action is pushed onto a stack . The most recent action is always at the top. If users decide to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2350] \\nundo an operation, the system pops the top element off the stack, effectively reversing the last action. This \\nimplementation provides a straightforward and efficient way to manage a history of user actions, offering a \\nseamless and intuitive undo functionality in applications ranging from text editors to graphic design software.  \\nReal-World Application: Web Browser Navigation'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='seamless and intuitive undo functionality in applications ranging from text editors to graphic design software.  \\nReal-World Application: Web Browser Navigation  \\nWhen you navigate through web pages using a browser, the stack data  structure is employed to manage the \\nhistory of visited pages and enable the \"Back\" and \"Forward\" functionalities. Each time you visit a new page, it \\nis pushed onto the stack. If you click the \"Back\" button, the browser pops the top page from the stack, \\neffectively taking you back to the previously visited page. Conversely, if you click the \"Forward\" button, the \\nbrowser pushes the next page onto the stack, allowing you to move forward through your browsing history. \\nThis stack-based approach provides a simpl e and efficient way to track and navigate through the sequence of \\nweb pages you have visited, enhancing the overall user experience in web browsing applications.  \\n \\nFig. 3: BASIC STRUCTURE OF STACK \\n4. Queues:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='web pages you have visited, enhancing the overall user experience in web browsing applications.  \\n \\nFig. 3: BASIC STRUCTURE OF STACK \\n4. Queues: \\nA queue is a fundamental data structure that adheres to the First In, First Out (FIFO) principle, serving as an \\nordered collection of elements where insertion occurs at the rear, and removal takes place at the front. This \\ndynamic structure operates like a real-world queue, where entities join at the back and are served or processed \\nfrom the front. Envisioned as a linear arrangement, each element in the queue, often referred to as a \"node,\" \\ncontains data and a reference to the next node in the sequence. Queu es find extensive application in scenarios \\nwhere tasks or data must be processed in the order they are received, such as print job scheduling in operating \\nsystems, managing tasks in asynchronous systems, or modeling processes in computer networks. The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='systems, managing tasks in asynchronous systems, or modeling processes in computer networks. The \\nsimplicity and efficiency of the FIFO mechanism make queues instrumental in scenarios demanding orderly and \\nsequential data processing, ensuring that the first element enqueued is the first to be dequeued, maintaining a \\nstructured flow of operations. Queues ar e widely employed in scenarios requiring orderly and sequential data \\nprocessing. One prominent real -world application is in print job scheduling within operating systems. Print \\njobs are enqueued as they are submitted to the printer queue, and they are proc essed in the order they are \\nreceived. This First In, First Out (FIFO) approach ensures fairness in task execution, preventing resource \\ncontention and providing an organized mechanism for handling print requests. Queues are integral in managing \\ntasks in a sequential manner, making them valuable in various systems where tasks need to be processed in the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='tasks in a sequential manner, making them valuable in various systems where tasks need to be processed in the \\norder they arrive, contributing to efficient and systematic data flow.  \\nReal-World Application: Customer Support Chat Queue  \\nIn online customer support system s, a queue is often employed to manage incoming customer queries in a fair \\nand organized manner. When customers initiate chat sessions for assistance, their requests join a queue, \\nforming a line based on the order of arrival. The customer service represent atives address inquiries in a First \\nIn, First Out (FIFO) fashion. This ensures that the earliest customer requests are attended to first, maintaining a \\nsense of fairness and timely responsiveness. The queue structure allows for a systematic approach to han dling \\ncustomer inquiries, preventing bottlenecks, and providing an efficient and orderly way to manage the flow of \\ncustomer support interactions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2351] \\n \\nFig. 4: BASIC STRUCTURE OF QUEUE \\n5. Trees: \\nA tree is a hierarchical and widely used data structure in computer  science that represents a collection of \\nelements organized in a branching structure. Comprising nodes connected by edges, a tree consists of a root \\nnode serving as the topmost element, and each node can have zero or more child nodes. Nodes in a tree are'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='node serving as the topmost element, and each node can have zero or more child nodes. Nodes in a tree are \\ninterconnected in a way that no circular paths exist, defining a directed acyclic graph. Nodes that share a \\ncommon parent are considered siblings, and those stemming from the same parent are referred to as subtrees. \\nTrees are characterized by their versatil ity and efficiency in representing hierarchical relationships, making \\nthem fundamental in various domains. They find applications in file systems where directories and files are \\norganized, database indexing for efficient data retrieval, search algorithms l ike binary search, and hierarchical \\ndata representation. With types such as binary trees, AVL trees, and Btrees, the hierarchical structure of trees \\nserves as a powerful paradigm for organizing and navigating complex relationships in diverse computational \\nscenarios.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='serves as a powerful paradigm for organizing and navigating complex relationships in diverse computational \\nscenarios.  \\nTrees are applied in various real -world scenarios due to their hierarchical structure. One notable application is \\nin file systems, where trees represent directories and files. The root node signifies the main directory, with \\nbranches extending to subdirectories and leaves representing individual files. This hierarchical organization \\nfacilitates efficient storage, retrieval, and navigation of files, exemplifying how trees enhance the structuring of \\ncomplex relationships. Trees are also pivotal in  database indexing, optimizing search algorithms, and \\nrepresenting hierarchical relationships in applications such as organizational charts and network routing. Their \\nversatility and efficiency make trees a foundational data structure for managing and stru cturing information in \\ndiverse computational environments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='versatility and efficiency make trees a foundational data structure for managing and stru cturing information in \\ndiverse computational environments.  \\nReal-World Application: Company Organizational Chart  \\nConsider a large company with multiple departments, teams, and hierarchical levels. The organizational \\nstructure of such a company can be effe ctively represented using a tree. The root node would symbolize the \\nCEO or the top management, and each subsequent level would represent different management tiers, \\ndepartments, teams, and individual employees. Nodes branching from a higher -level node repr esent the \\nreporting structure, where employees report to their immediate superiors. This tree structure not only mirrors \\nthe organizational hierarchy but also simplifies tasks such as finding reporting relationships, understanding \\ndepartmental structures, and facilitating efficient communication within the company. The use of a tree in this'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"departmental structures, and facilitating efficient communication within the company. The use of a tree in this \\ncontext provides a clear and visual representation of the company's organizational framework, aiding in \\ndecision-making, communication, and overall management. \\n \\nFig. 5: BASIC STRUCTURE OF TREE \\n6. Graphs: \\nA graph is a versatile and fundamental data structure in computer science, consisting of a set of nodes (vertices) \\ninterconnected by edges. These edges represent relationships or connections between nodes, and they can be\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2352] \\neither directed or undirected. Graphs can take various forms, including directed graphs (digraphs), where \\nedges have a specific direction, and undirected graphs, where edges have no direction.  \\nNodes in a graph may also have weights or labels, adding additional information to the relationships.  \\nGraphs find broad applications in modeling complex relationships and networks, ranging from social networks'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"Graphs find broad applications in modeling complex relationships and networks, ranging from social networks \\nand transportation systems to computer networks and project management. Graph algorithms, such as \\nDijkstra's algorithm and breadth -first search, are essential tools for analyzing and solving problems in areas \\nlike route optimization, network flow, and recommendation systems. The flexibility and adaptability of graphs \\nmake them a powerful representation for capt uring and analyzing intricate relationships in diverse \\ncomputational domains.  \\nGraphs are widely employed in various real -world applications due to their ability to model complex \\nrelationships. One notable application is in social networks, where individua ls are represented as nodes, and \\nconnections between them as edges. Graphs enable the analysis of social interactions, identification of \\ninfluential nodes, and the prediction of network behavior. They also play a crucial role in logistics and\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='influential nodes, and the prediction of network behavior. They also play a crucial role in logistics and \\ntransportation systems, where nodes represent locations and edges denote routes. Graph algorithms are \\nutilized for optimizing transportation routes, managing network flow, and enhancing efficiency in diverse \\nscenarios, from package delivery to urban planning. The adapt ability of graphs makes them a versatile tool for \\ncapturing and understanding intricate relationships in dynamic systems.  \\nReal-World Application: Social Network Analysis  \\nConsider a social network, such as Facebook or LinkedIn, where individuals are repre sented as nodes and \\nrelationships between them as edges. This network can be modeled as a graph, with each person being a node, \\nand connections (friendships or professional relationships) forming the edges. Graph algorithms can then be \\napplied to analyze t he structure of the social network, identify key influencers, predict connections, and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='applied to analyze t he structure of the social network, identify key influencers, predict connections, and \\nunderstand the overall connectivity patterns. Social network analysis using graphs has applications in various \\nfields, from targeted advertising and content recommendati on to understanding the spread of information and \\ninfluence within online communities. The graph representation provides a powerful tool for gaining insights \\ninto the dynamics and relationships within complex social systems.  \\n \\nFig. 6: BASIC STRUCTURE OF GRAPH \\n7. Hash Tables: \\nA hashtable, or hash map, is a fundamental data structure that facilitates efficient data retrieval by associating \\nkeys with corresponding values through a process called hashing. It employs a hash function to convert keys \\ninto indices, where the associated values are stored in an array -like structure called a bucket. The key feature of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='into indices, where the associated values are stored in an array -like structure called a bucket. The key feature of \\na hashtable is its ability to provide constant -time average -case complexity for common operations, such as \\ninsertion, retrieval, and deletion, by minimiz ing collisions —situations where multiple keys hash to the same \\nindex. Collision resolution methods, such as chaining or open addressing, ensure that each bucket can store \\nmultiple key-value pairs. Hashtables find wide application in diverse computing scena rios, including database \\nindexing, symbol tables in compilers, and spell checkers, where the efficient retrieval of data based on keys is \\ncrucial for optimizing algorithmic performance. The adaptability and efficiency of hashtables make them \\nintegral in addressing challenges related to data access and retrieval in various computational domains.  \\nHashtables are extensively applied in various real -world scenarios, notably in database systems for efficient'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Hashtables are extensively applied in various real -world scenarios, notably in database systems for efficient \\ndata retrieval. In this context, keys, representing un ique identifiers or attributes, are hashed using a hash \\nfunction, and the resulting indices point to corresponding values stored in the hashtable. This ensures swift \\naccess to specific data entries, significantly reducing retrieval time compared to linear search methods. \\nHashtables play a pivotal role in optimizing the performance of database systems, making them indispensable'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2353] \\nfor tasks such as quick data lookup, indexing, and retrieval in applications ranging from search engines to \\nfinancial systems, where speed and efficiency in accessing and managing data are critical.  \\nReal-World Application: Spell Checking in Text Editors  \\nHashtables are commonly employed in spell checkers within text editors to ensure efficient and rapid word'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"Real-World Application: Spell Checking in Text Editors  \\nHashtables are commonly employed in spell checkers within text editors to ensure efficient and rapid word \\nlookup. In this application, each word in a dictionary is associated with a unique key using a hash function. The \\nhashtable then stores these keys along with their corresponding words. During spell checking, when a user \\ninputs a word, the spell checker hashes the word to find its corresponding key and quickly checks if it exists in \\nthe hashtable. This process allows for near -instantaneous verification of the word's correctness. Hashtables are \\npivotal in this context because they provide constant -time averagecase complexity  for key -based operations, \\nensuring a swift and responsive spell -checking process, which is crucial for enhancing the user experience in \\ntext editing applications. \\n8. Heaps: \\nA heap is a specialized tree -based data structure that satisfies the heap property, which distinguishes it as\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"text editing applications. \\n8. Heaps: \\nA heap is a specialized tree -based data structure that satisfies the heap property, which distinguishes it as \\neither a max -heap or a min -heap. In a max -heap, for any given node, the value of the node is greater than or \\nequal to the values of its children, ensuring that the maximum element is at the root. Conversely, in a min -heap, \\nthe value of each node is less than or equal to the values of its children, making the minimum element the root. \\nHeaps are typically implemented as binary trees, where the relationship between parent and child nodes is \\nmaintained by the heap property. The key fea ture of heaps lies in their ability to provide efficient access to the \\nmaximum or minimum element, making them valuable in priority queue implementations. Heaps find \\napplications in various algorithms, including heap sort, Dijkstra's algorithm for shortest  paths, and in -memory\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"applications in various algorithms, including heap sort, Dijkstra's algorithm for shortest  paths, and in -memory \\nsorting operations, where the logarithmic height of the heap ensures fast retrieval and manipulation of extreme \\nvalues. The versatility and performance characteristics of heaps make them a fundamental data structure in \\nalgorithmic design and optimization.  \\nOne notable application of heaps is in priority queues, where elements are assigned priorities, and the highest \\n(or lowest) priority can be efficiently accessed and removed. This is crucial in scenarios such as task scheduling, \\nemergency room triage, and network routing, where tasks, patients, or data packets need to be processed based \\non priority levels. Heaps enable quick identification of the highest -priority element, ensuring that critical \\noperations are performed first. This vers atility makes heaps integral in optimizing resource allocation and task\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='operations are performed first. This vers atility makes heaps integral in optimizing resource allocation and task \\nexecution in various real-world applications where prioritization is a key factor.  \\nReal-World Application: Emergency Room Triage  \\nConsider an emergency room (ER) scenario where patien ts arrive with varying degrees of medical urgency. A \\nheap-based priority queue can be employed to manage the triage process efficiently. Each patient is assigned a \\npriority based on the severity of their condition, with the highest priority given to the mo st critical cases. As \\nnew patients arrive, their information, including the severity of their condition, is added to the priority queue \\nimplemented as a max -heap. The patient with the highest priority (most critical condition) is quickly identified \\nand att ended to by the medical staff. This ensures that critical cases are addressed promptly, reflecting the \\nurgency and prioritization inherent in healthcare settings.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='and att ended to by the medical staff. This ensures that critical cases are addressed promptly, reflecting the \\nurgency and prioritization inherent in healthcare settings.  \\nThe heap structure allows for constant -time retrieval of the patient with the highest priori ty, facilitating a \\nstreamlined triage process and optimizing the allocation of medical resources in emergency situations.  \\n                 \\n                        Fig. 7: BASIC STRUCTURE OF MAX HEAP          Fig. 8: BASIC STRUCTURE OF MIN HEAP'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2354] \\nOOP \\nObject-Oriented Programming (OOP) is a programming paradigm that structures code around the concept of \\nobjects, which encapsulate data and behavior. OOP principles include encapsulation, where data and methods \\nare bundled within objects, enhancing code mod ularity. Inheritance allows the creation of new classes by \\ninheriting properties and methods from existing ones, promoting code reuse. Polymorphism enhances'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"inheriting properties and methods from existing ones, promoting code reuse. Polymorphism enhances \\nflexibility by enabling objects of diverse types to be treated as instances of a shared type.  Abstraction allows the \\nrepresentation of complex systems by focusing on essential properties and behaviors. OOP fosters code \\norganization, making it more intuitive and maintainable. Objects communicate through defined interfaces, \\nenhancing modularity and reduci ng dependencies. OOP is widely used in software development for modeling \\nreal-world entities and designing scalable, reusable, and modular code. Java, C++, and Python are popular OOP \\nlanguages. OOP promotes the design of software systems that reflect real -world structures and relationships. \\nHere's an overview of these concepts and other aspects of Object-Oriented Programming:  \\n1. Objects and Classes:  \\nClass: A blueprint or template that defines the properties and behaviors common to all objects of a certai n type.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='1. Objects and Classes:  \\nClass: A blueprint or template that defines the properties and behaviors common to all objects of a certai n type.  \\nObject: An instance of a class, representing a concrete realization of the class blueprint.  \\n2. Encapsulation:  \\nEncapsulation involves bundling the data (attributes) and methods (functions) that operate on the data within \\na single unit, known as a class.  \\nThis shields the internal implementation details from the outside world, allowing the object to control access to \\nits data and methods.  \\n3. Inheritance:  \\nInheritance is a mechanism that allows a new class (subclass or derived class) to inherit attri butes and \\nbehaviors from an existing class (superclass or base class).  \\nIt promotes code reusability and establishes an \"is-a\" relationship between the subclasses and the superclass.  \\n4. Polymorphism:  \\nPolymorphism permits the treatment of objects from vari ous classes as instances of a common base class . It'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='4. Polymorphism:  \\nPolymorphism permits the treatment of objects from vari ous classes as instances of a common base class . It \\nincludes method overloading (multiple methods with the same name but different parameters) and method \\noverriding (providing a specific implementation for a method in a subclass).  \\n5. Abstraction:  \\nAbstraction simplifies intricate systems by modeling classes according to the essential properties and \\nbehaviors pertinent to the specific problem.  It helps in managing software complexity by focusing on high -level \\nconcepts.  \\n6. Modularity:  \\nOOP promotes modularity by breaking down a system into smaller, independent, and interchangeable modules \\n(classes). Each module can be developed, tested, and maintained separately, contributing to a more organized \\nand scalable codebase.  \\n7. Encapsulation, Inheritance, and Polymorphism (EIP):  \\nTogether, encapsulation, inheritance, and polymorphism form the core principles of OOP, commonly known as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='and scalable codebase.  \\n7. Encapsulation, Inheritance, and Polymorphism (EIP):  \\nTogether, encapsulation, inheritance, and polymorphism form the core principles of OOP, commonly known as \\nEIP. EIP provides a foundation for building flexible, maintainable, and extensible software systems.  \\n8. Class Relationships:  \\nAssociations: Connections between classes, such as one-to-one, one-to-many, and many to many relationships.  \\nAggregation: A type of association where one class represents a \"whole\" and another class represents a \"part.\"  \\nComposition: A stronger form of aggregation where the \"part\" is tightly bound to the \"whole.\"  \\n9. Design Patterns:  \\nDesign patterns are repetitive solutions addressing prevalent challenges in software design.  OOP supports the \\nimplementation of design patterns, such as Singleton, Factory, Obs erver, and MVC (Model -View-Controller), to \\nimprove code organization and maintainability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2355] \\nOOP is widely used in software development due to its ability to model real -world entities, enhance code \\norganization, and promote code reuse. Popular OOP languages  include Java, C++, Python, and C#.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Banking System:  \\nIn an online banking system, Object -Oriented Programming is commonly employed to model entities such as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Some of the Real-World Applications of OOP are as follows:  \\n1. Online Banking System:  \\nIn an online banking system, Object -Oriented Programming is commonly employed to model entities such as \\naccounts, transactions, and c ustomers. Each of these entities can be represented as objects with specific \\nproperties (account balance, transaction history) and behaviors (transfer funds, check balance). Inheritance can \\nbe used to represent different types of accounts, while encapsulat ion ensures that sensitive data is hidden and \\nonly accessible through defined methods.  \\n2. E-commerce Platform:  \\nE-commerce platforms utilize OOP to model products, orders, customers, and the shopping cart. Each product \\ncan be an object with properties like  price and description. The shopping cart can be implemented as an object \\nthat manages the addition and removal of products. Inheritance can be applied to represent different types of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='that manages the addition and removal of products. Inheritance can be applied to represent different types of \\nproducts or discounts, and polymorphism allows for a flexible checkout p rocess.  \\n3. Video Game Development:  \\nIn the gaming industry, OOP is extensively used to model game entities like characters, enemies, weapons, and \\nenvironments. Each game object is represented as a class with specific attributes and behaviors. Inheritance \\ncan be employed to create variations of characters or enemies, and polymorphism allows for dynamic \\ninteractions between different game objects.  \\n4. Hospital Information System:  \\nHospital information systems often use OOP to model patients, doctors, appointme nts, and medical records. \\nEach entity can be represented as an object with relevant attributes and methods. Inheritance can be employed \\nto represent different types of medical professionals, and polymorphism can facilitate the integration of various \\nspecialized modules within the system.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='to represent different types of medical professionals, and polymorphism can facilitate the integration of various \\nspecialized modules within the system.  \\n5. Social Media Platform:  \\n \\nFig. 9: ARCHITECTURE OF OOP \\nSocial media platforms extensively leverage OOP to model users, posts, comments, and interactions. Each user \\ncan be represented as an object with properties like username and profile information. Posts and comments \\ncan also be modeled as objects with associated behaviors. Inheritance can be applied to represent different'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2356] \\ntypes of users (regular users, administrators), and polymorphism allows for the dynamic handling of various \\ncontent types. These examples illustrate how Object -Oriented Programming is applied in diverse real -world \\nscenarios to model and manage complex systems by organizing code into modular and reusable structures. \\nOOP provides a scalable and maintainable approach to software develo pment, enabling the creation of systems'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='OOP provides a scalable and maintainable approach to software develo pment, enabling the creation of systems \\nthat mimic real-world entities and interactions. \\nDBMS \\nA Database Management System (DBMS) is a software tool designed to aid in the generation, administration, \\nand modification of databases. DBMS serves as an interface between the user and the database, providing tools \\nfor data definition, storage, retrieval, and manipulation. It ensures data integrity through features like \\nconstraints, enforcing rules on data stored in databases. D BMS supports the implementation of complex queries \\nand transactions, allowing users to interact with databases efficiently. It provides mechanisms for data security, \\nincluding user authentication, authorization, and encryption. DBMS allows for concurrent a ccess by multiple \\nusers while maintaining consistency and isolation of data. Common types of DBMS include relational, NoSQL,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='users while maintaining consistency and isolation of data. Common types of DBMS include relational, NoSQL, \\nand object-oriented databases. SQL (Structured Query Language) is often used to interact with relational DBMS. \\nDBMS plays a crucial  role in various applications, from business systems to web development, by providing a \\nstructured and efficient means of managing and accessing data.  \\nHere are key aspects of Database Management Systems:  \\n1. Data Definition Language (DDL):  \\nDDL is a subset  of SQL (Structured Query Language) that allows users to define the structure of the database, \\nincluding creating, altering, and deleting tables and establishing relationships between them.  \\n2. Data Manipulation Language (DML):  \\nDML enables users to interac t with the data stored in the database. Common DML operations include inserting, \\nupdating, and deleting records, as well as querying data using SELECT statements.  \\n3. Data Integrity:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='updating, and deleting records, as well as querying data using SELECT statements.  \\n3. Data Integrity:  \\nDBMS ensures data integrity by enforcing constraints such as primary keys , foreign keys, unique constraints, \\nand check constraints. These restrictions are in place to avoid the storage of data that is either invalid or \\ninconsistent in the database. \\n4. Concurrency Control:  \\nConcurrency control mechanisms in DBMS manage simultaneo us access to data by multiple users to ensure \\ndata consistency. Techniques like locking and transaction isolation levels are employed to prevent conflicts.  \\n5. Transaction Management:  \\nDBMS supports transactions, which are sequences of one or more SQL opera tions treated as a single unit of \\nwork. Transactions ensure data consistency by either committing changes if successful or rolling back if an \\nerror occurs.  \\n6. Data Security:  \\nDBMS provides mechanisms for securing data, including user authentication, author ization, and access control.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='error occurs.  \\n6. Data Security:  \\nDBMS provides mechanisms for securing data, including user authentication, author ization, and access control. \\nUsers are granted specific permissions to perform operations on certain data, preventing unauthorized access.  \\n7. Data Modeling:  \\nDBMS enables the creation of data models to represent the structure and relationships within the database. \\nTypical data models encompass the relational model, hierarchical model, network model, and object -oriented \\nmodel. \\n8. Query Optimization:  \\nDBMS optimiz es queries to enhance performance. Query optimization involves selecting the most efficient \\nexecution plan for a given query, considering factors like indexes, join algorithms, and access methods.  \\n9. Normalization:  \\nNormalization refers to the method of st ructuring data to minimize redundancy and enhance data integrity.  \\nThis involves decomposing tables and ensuring that data dependencies are minimized.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2357] \\n10. Backup and Recovery:  \\nDBMS provides tools for creating backups of the database to prevent data loss in  case of hardware failure, \\nsoftware errors, or other disasters. Recovery mechanisms restore the database to a consistent state after a \\nfailure.  \\n11. Scalability:  \\nDBMS systems are designed to scale, allowing for the management of large datasets and handling  a growing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='failure.  \\n11. Scalability:  \\nDBMS systems are designed to scale, allowing for the management of large datasets and handling  a growing \\nnumber of concurrent users. Scalability is crucial for accommodating expanding data requirements.  \\n12. Data Warehousing and Data Mining:  \\nDBMS is often integrated with data warehousing and data mining tools to support the extraction, \\ntransformation, and loading (ETL) of data, as well as the analysis and discovery of patterns and trends in large \\ndatasets.  \\nPopular Database Management Systems  include MySQL, Oracle Database, Microsoft SQL Server, PostgreSQL, \\nand MongoDB. The choice of a DBMS depends on the specific requirements of the application, data model \\npreferences, and scalability needs.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Retail and E-commerce Platforms:  \\nOnline retail platforms, such as Amazon and eBay, heavily rely on DBMS to manage vast product catalogs,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='1. Online Retail and E-commerce Platforms:  \\nOnline retail platforms, such as Amazon and eBay, heavily rely on DBMS to manage vast product catalogs, \\ncustomer information, order processing, and inventory. The system ensures quick and accurate retrie val of \\nproduct information, tracks customer orders, and supports seamless transactions.  \\n2. Airline Reservation Systems:  \\nAirline reservation systems, like those used by airlines worldwide, utilize DBMS to manage flight schedules, \\nseat availability, passeng er information, and ticketing. This ensures efficient booking processes, tracks \\npassenger details, and allows for dynamic adjustments to flight schedules.  \\n3. Human Resource Management Systems (HRMS):  \\nHRMS applications, used by companies for personnel mana gement, leverage DBMS to store employee records, \\npayroll information, attendance data, and performance evaluations. The system ensures the secure and \\norganized management of HR-related information.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='payroll information, attendance data, and performance evaluations. The system ensures the secure and \\norganized management of HR-related information.  \\n4. Social Media Platforms:  \\nSocial media platforms, includ ing Facebook, Twitter, and Instagram, employ DBMS to manage user profiles, \\nposts, comments, and social connections. The system enables rapid retrieval of personalized content, tracks \\nuser interactions, and supports features like recommendations and targete d advertising.  \\n5. Library Management Systems:  \\nLibrary management systems, used by educational institutions and public libraries, utilize DBMS to organize \\nand catalog books, manage borrower information, and track lending transactions. The system ensures ef ficient \\nlibrary operations, including book searches, checkouts, and returns.  \\nThese examples showcase the diverse applications of DBMS in different industries, highlighting its role in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='library operations, including book searches, checkouts, and returns.  \\nThese examples showcase the diverse applications of DBMS in different industries, highlighting its role in \\norganizing, storing, and retrieving data to support critical business functions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2358] \\n \\nFig. 10: ARCHITECTURE OF DBMS \\nOS \\nAn Operating System (OS) is a software that acts as an intermediary between computer hardware and user \\napplications, managing hardware resources and facilitating seamless execution of software. It provides \\nessential services like process management, handli ng the creation, scheduling, and termination of processes.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='essential services like process management, handli ng the creation, scheduling, and termination of processes. \\nMemory management allocates and deallocates memory space for processes, ensuring efficient utilization. File \\nsystem management organizes and stores data on storage devices, allowing for file creati on, deletion, and \\nmanipulation. Device management facilitates communication between the OS and hardware devices, such as \\ninput/output peripherals. Security features include user authentication, access controls, and encryption to \\nsafeguard the system. The O S provides a user interface (UI), which can be command -line or graphical, enabling \\nuser interaction. Networking capabilities support communication between computers and devices within a \\nnetwork. Error handling mechanisms address issues and maintain system stability. The OS is fundamental for \\nthe execution of diverse software applications and is integral to the functionality of computers and devices.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='the execution of diverse software applications and is integral to the functionality of computers and devices.  \\nHere are key aspects of Operating Systems:  \\n1. Kernel:  \\nThe kernel constitutes the central element of the ope rating system.  It provides essential services, including \\nprocess management, memory management, device management, and system calls. The kernel interacts \\ndirectly with the hardware and ensures that different software components can run efficiently.  \\n2. Process Management:  \\nProcess management encompasses the initiation, scheduling, and cessation of processes.  The OS manages the \\nexecution of multiple processes concurrently, allowing users to run multiple applications simultaneously.  \\n3. Memory Management:  \\nMemory management is responsible for allocating and deallocating memory space for processes. The OS \\nensures efficient utilization of memory, handles memory protection, and facilitates virtual memory to extend \\navailable RAM.  \\n4. File System Management:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='ensures efficient utilization of memory, handles memory protection, and facilitates virtual memory to extend \\navailable RAM.  \\n4. File System Management:  \\nThe OS m anages file systems, organizing and storing data on storage devices such as hard drives. It provides \\nfile-related services, including file creation, deletion, reading, and writing. File systems organize data into \\ndirectories and files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2359] \\n5. Device Management:  \\nDevice management handles communication between the OS and hardware devices, including input/output \\ndevices like keyboards, printers, and storage devices. It provides drivers and interfaces to enable \\ncommunication and control of these devices.  \\n6. Security and Protection:  \\nThe operating system enforces security protocols to safeguard both the system and user data.  This includes'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='communication and control of these devices.  \\n6. Security and Protection:  \\nThe operating system enforces security protocols to safeguard both the system and user data.  This includes \\nuser authentication, access controls, encryption, and security patches to address vulnerabilities. The OS also \\nenforces memory protection to prevent unauthorized access to memory areas.  \\n7. User Interface:  \\nThe user interface (UI) provides a means for users to interact with the computer. OS can have a command -line \\ninterface (CLI) or a graphical user interface (GUI). GUIs include elem ents like windows, icons, menus, and \\nbuttons to enhance user experience.  \\n8. Networking:  \\nNetworking capabilities enable computers to communicate with each other in a network. The OS provides \\nnetworking protocols and services, facilitating tasks such as fil e sharing, internet access, and remote access to \\nresources.  \\n9. Schedulers:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"networking protocols and services, facilitating tasks such as fil e sharing, internet access, and remote access to \\nresources.  \\n9. Schedulers:  \\nSchedulers manage the execution of processes and allocate CPU time. They include process schedulers for \\nshort-term CPU scheduling, and long -term schedulers for selecting processes to be brought into the ready \\nqueue.  \\n10. Error Handling:  \\nOS handles errors and exceptions to maintain system stability. It provides error messages, logs, and recovery \\nmechanisms to address issues that may arise during system operation.  \\n11. System Calls:  \\nSystem calls are interfaces through which user -level programs request services from the operating system. \\nCommon system calls include file operations, process control, and memory management.  \\n12. Boot Process:  \\nThe boot process initializes the computer's har dware and loads the OS into memory. It involves the BIOS/UEFI, \\nbootloader, and kernel, leading to the full initialization of the OS.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"The boot process initializes the computer's har dware and loads the OS into memory. It involves the BIOS/UEFI, \\nbootloader, and kernel, leading to the full initialization of the OS.  \\nOperating systems come in various types, including Windows, macOS, Linux, Unix, and others. Each serve as a \\nfundamental layer in the computing stack, enabling the execution of applications and providing a user -friendly \\nenvironment for users to interact with their devices.  \\nHere are two examples of real-world applications that extensively rely on Operating Systems:  \\n1. Automated Teller Machines (ATMs):  \\nAutomated Teller Machines, commonly found in banks and other financial institutions, heavily depend on \\noperating systems for their functionality. The OS manages interactions between the ATM hardware \\ncomponents, such as the card reader, cash dispenser, and keypad. It handles security features, user\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"operating systems for their functionality. The OS manages interactions between the ATM hardware \\ncomponents, such as the card reader, cash dispenser, and keypad. It handles security features, user \\nauthentication, and communication with the bank's servers. The OS ensures that transactions are processed \\nsecurely and efficiently, providing users with a seamless and reliable  experience.  \\n2. Smartphones:  \\nSmartphones, including those running iOS (Apple), Android, or other mobile operating systems, are ubiquitous \\ndevices that rely on operating systems to manage a wide range of functions. The OS on a smartphone handles \\nthe user interface, application management, memory allocation, power management, and communication with \\nvarious hardware components (e.g., camera, GPS, sensors). It ensures a smooth user experience by coordinating \\nthe execution of diverse applications  and services, making smartphones an integral part of everyday life. These\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='the execution of diverse applications  and services, making smartphones an integral part of everyday life. These \\nexamples illustrate how operating systems play a crucial role in diverse technological applications, enabling the \\neffective and secure functioning of complex systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2360] \\n \\nFig. 11: ARCHITECTURE OF OS \\nIII. METHODOLOGY \\nIn the landscape of computer science, the integration and interconnectivity among Data Structures and \\nAlgorithms (DSA), Object -Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) form the backbone of s ophisticated software development and system design. Each of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"Operating Systems (OS) form the backbone of s ophisticated software development and system design. Each of \\nthese fundamental subjects contributes distinct elements to the overall architecture of a computing system, and \\ntheir seamless integration is essential for creating robust and efficient applicati ons.  \\nHere's an exploration of how these subjects intersect:  \\n1. DSA in System Design:  \\nData Structures and Algorithms play a pivotal role in designing efficient and scalable systems. DSA is the \\nbedrock for optimizing data storage, retrieval, and manipulati on. Algorithms, whether for sorting, searching, or \\noptimizing, are crucial for creating performant applications. In conjunction with DSA, system architects can \\ndesign data models that align with the requirements of both DBMS and OOP, ensuring effective uti lization of \\nresources.  \\n2. OOP for Code Organization and Reusability:  \\nObject-Oriented Programming provides a paradigm for organizing code into modular and reusable\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='resources.  \\n2. OOP for Code Organization and Reusability:  \\nObject-Oriented Programming provides a paradigm for organizing code into modular and reusable \\ncomponents. Objects encapsulate data and behavior, facilitating the creation of software ent ities that model \\nreal-world entities. In an integrated system, OOP principles enable clean interfaces and interactions between \\ndifferent modules. DSA structures can be seamlessly integrated into OOP -designed systems, allowing for \\nefficient data manipulation and algorithm execution within the context of object-oriented applications.  \\n3. DBMS for Persistent Data Storage:  \\nDatabase Management Systems are central to storing and retrieving data persistently. In an integrated system, \\nthe interaction between DSA and DBMS is evident in how data structures are translated into database schemas.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2361] \\nDSA concepts like indexing and normalization directly impact how data is organized in a database. The system's \\narchitecture, shaped by DSA, ensures that data is efficiently managed and manipulated through DBMS, \\nproviding a structured and scalable approach to data storage.  \\n4. OS for Resource Management and Execution:  \\nThe Operating System acts as the underlying platform that orchestrates the execution of software applications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='4. OS for Resource Management and Execution:  \\nThe Operating System acts as the underlying platform that orchestrates the execution of software applications. \\nIn an integrated system, the OS interacts with applications designed using OOP principles, allocates memory \\nbased on DS A requirements, and manages access to data stored in DBMS. The OS ensures the efficient \\nutilization of hardware resources, providing a foundation for the execution of diverse software components.  \\nExamples of Interconnectivity:  \\n1. E-commerce Platform:  \\nAn e -commerce platform exemplifies the interconnectivity of DSA, OOP, DBMS, and OS. DSA algorithms are \\nemployed for efficient search and recommendation systems. OOP principles are applied for organizing code \\ninto modular components. DBMS is used to store a nd retrieve product information, user data, and transaction \\nrecords. The OS ensures the secure and efficient execution of the entire system.  \\n2. Healthcare Information System:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='records. The OS ensures the secure and efficient execution of the entire system.  \\n2. Healthcare Information System:  \\nIn a healthcare information system, DSA is used for optimizing patient data str uctures, OOP principles guide the \\norganization of healthcare modules, DBMS manages patient records, and the OS ensures the secure and reliable \\noperation of the entire system. The interplay of these fundamental subjects is crucial for maintaining the \\nintegrity and efficiency of healthcare information systems. In essence, the integration of DSA, OOP, DBMS, and \\nOS is not merely a theoretical concept but a practical necessity for building sophisticated, interconnected, and \\nhigh-performance computing systems. Un derstanding how these subjects complement each other is essential \\nfor professionals and students alike, as it lays the groundwork for effective problem -solving and innovation in \\nthe ever-evolving landscape of computer science. \\nIV. CONCLUSION'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='for professionals and students alike, as it lays the groundwork for effective problem -solving and innovation in \\nthe ever-evolving landscape of computer science. \\nIV. CONCLUSION \\nIn conclusion, th is paper underscores the foundational significance of Data Structures and Algorithms (DSA), \\nObject-Oriented Programming (OOP), Database Management Systems (DBMS), and Operating Systems (OS) in \\nthe realm of computer science. The paper emphasizes that a stro ng and comprehensive understanding of these \\nfundamental subjects is not only beneficial but essential for success in the dynamic and ever -evolving field of \\ncomputer science. Whether for students embarking on their educational journey or professionals navig ating the \\nintricacies of software development, the knowledge gained in DSA, OOP, DBMS, and OS serves as a bedrock for \\nbuilding innovative solutions, tackling challenges, and contributing meaningfully to the advancement of \\ntechnology. \\nV. REFERENCES'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='building innovative solutions, tackling challenges, and contributing meaningfully to the advancement of \\ntechnology. \\nV. REFERENCES \\n[1] R. K. Vosi lovich, \"Universal Linear Data Structure,\" 2020 International Conference on Information \\nScience and Communications Technologies (ICISCT), Tashkent, Uzbekistan, 2020, pp. 1 -3, doi: \\n10.1109/ICISCT50599.2020.9351485.  \\n[2] Donald Knuth. The Art of Computer Program ming, Volume 1: Fundamental Algorithms, Third Edition. \\nAddison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.2.1: Stacks, Queues, and Deques, pp. 238–243.  \\n[3] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to \\nAlgorithms, Second Edition. MIT Press and McGraw -Hill, 2001. ISBN 0 -262-03293-7. Section 10.1: \\nStacks and queues, pp. 200–204.  \\n[4] E. Vaahedi and K. W. Cheung, \"Evolution and future of on -line DSA,\" 2000 IEEE Power Engineering \\nSociety Winter Meeting. Conference Proce edings (Cat. No.00CH37077), Singapore, 2000, pp. 63 -65'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='[4] E. Vaahedi and K. W. Cheung, \"Evolution and future of on -line DSA,\" 2000 IEEE Power Engineering \\nSociety Winter Meeting. Conference Proce edings (Cat. No.00CH37077), Singapore, 2000, pp. 63 -65 \\nvol.1, doi: 10.1109/PESW.2000.849928.  \\n[5] T. Mudner and E. Shakshuki, \"A new approach to learning algorithms,\" International Conference on \\nInformation Technology: Coding and Computing, 2004. Proceedings. ITCC 2004., Las Vegas, NV, USA, \\n2004, pp. 141-145 Vol.1, doi: 10.1109/ITCC.2004.1286440.  \\n[6] Summer Meeting. Conference Proceedings (Cat. No.01CH37262), Vancouver, BC, Canada, 2001, pp. \\n1070-1074 vol.2, doi: 10.1109/PESS.2001.970207.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                                                                                                                    \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2362] \\n[7] B. Stroustrup, \"What is object -oriented programming?,\" in IEEE Software, vol. 5, no. 3, pp. 1020, May \\n1988, doi: 10.1109/52.2020.  \\n[8] G. Schlageter et al., \"OOPS -an object oriented programming system with integrated data management \\nfacility,\" Proceedings. Fourth International Conference on Data Engineering, Los Angeles, CA, USA, \\n1988, pp. 118-125, doi: 10.1109/ICDE.1988.105453.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='facility,\" Proceedings. Fourth International Conference on Data Engineering, Los Angeles, CA, USA, \\n1988, pp. 118-125, doi: 10.1109/ICDE.1988.105453. \\n[9] S. Abbasi, H. Kazi and K. Khowaja, \"A systematic review of learning object oriented programming \\nthrough serious games and programming approaches,\" 2017 4th IEEE International  Conference on \\nEngineering Technologies and Applied Sciences (ICETAS), Salmabad, Bahrain, 2017, pp. 1-6,  \\ndoi: 10.1109/ICETAS.2017.8277894. \\n[10] R. J. D\\'Andrea and R. G. Gowda, \"Object -oriented programming: concepts and languages,\" IEEE \\nConference on Aerospace and Electronics, Dayton, OH, USA, 1990, pp. 634-640 vol.2,  \\ndoi: 10.1109/NAECON.1990.112840.  \\n[11] S. Samaiya and M. Agarwal, \"Real time database management system,\" 2018 2nd International \\nConference on Inventive Systems and Control (ICISC), Coimbatore, India, 2018, pp. 903 -908,  \\ndoi: 10.1109/ICISC.2018.8398931.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Conference on Inventive Systems and Control (ICISC), Coimbatore, India, 2018, pp. 903 -908,  \\ndoi: 10.1109/ICISC.2018.8398931.  \\n[12] Wiederhold, \"Databases,\" in Computer, vol. 17, no. 10, pp. 211-223, Oct. 1984,  \\ndoi: 10.1109/MC.1984.1658971.  \\n[13] P. G. Selinger, \"Database technology,\" in IBM Systems Journal, vol. 26, no. 1, pp. 96 -106, 1987,  \\ndoi: 10.1147/sj.261.0096.  \\n[14] A. Corallo, M. Espostito, A . Massafra and S. Totaro, \"A Relational Database Management System \\nApproach for Data Integration in Manufacturing Process,\" 2018 IEEE International Conference on \\nEngineering, Technology and Innovation (ICE/ITMC), Stuttgart, Germany, 2018, pp. 1 -7,  \\ndoi: 10.1109/ICE.2018.8436290.  \\n[15] S. H. Son, \"Real -time database systems: present and future,\" Proceedings Second International \\nWorkshop on Real-Time Computing Systems and Applications, Tokyo, Japan, 1995, pp. 50 -52,  \\ndoi: 10.1109/RTCSA.1995.528750.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='Workshop on Real-Time Computing Systems and Applications, Tokyo, Japan, 1995, pp. 50 -52,  \\ndoi: 10.1109/RTCSA.1995.528750.  \\n[16] R. R. Muntz, \"Operating systems,\" in Computer, vol. 7, no. 6, pp. 21-21, June 1974,  \\ndoi: 10.1109/MC.1974.6323579.  \\n[17] W. Chengjun, \"The Analyses of Operating System Structure,\" 2009 Second International Symposium on \\nKnowledge Acquisition and Modeling, Wuhan, China, 2009, pp. 354-357, doi: 10.1109/KAM.2009.265.  \\n[18] Norman F. Schneidewind, \"Operating Systems,\" in Computer, Network, Software, and Hardware \\nEngineering with Applications , IEEE, 2012, pp.286-302, doi: 10.1002/9781118181287.ch10.  \\n[19] H. Mei and Y. Guo, \"Operating Systems for Internetware: Challenges and Future Directions,\" 2018 IEEE \\n38th International Conference on Distributed Computing Systems (ICDCS), Vienna, Austria, 2018, pp. \\n1377-1384, doi: 10.1109/ICDCS.2018.00138.  \\n[20] C. Peng, X. -q. Yang, Z. -z. Niu and X. -p. Liu, \"R esearch on Windows operating system education,\" 2009'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'author': 'Rampyari', 'moddate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'source_file': 'cs_concepts.pdf', 'file_type': 'pdf'}, page_content='1377-1384, doi: 10.1109/ICDCS.2018.00138.  \\n[20] C. Peng, X. -q. Yang, Z. -z. Niu and X. -p. Liu, \"R esearch on Windows operating system education,\" 2009 \\nIEEE International Symposium on IT in Medicine & Education, Jinan, China, 2009, pp. 719 -724, doi: \\n10.1109/ITIME.2009.5236327.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\",\"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents (documents)\n",
    "    print (f\"Split {len (documents)} documents into {len(split_docs)} chunks\")\n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk: \")\n",
    "        print (f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print (f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs\n",
    "\n",
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c54ad",
   "metadata": {},
   "source": [
    "### Embeddings and VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5afb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d97b11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x7ffa540f6bf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        Args:\n",
    "            model_name: Hugging Face model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the Sentence Transformer model\"\"\"\n",
    "        try:\n",
    "            print (f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print (f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print (f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        print (f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print (f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3142809",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842fab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x7ffa5074c4c0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "        persist_directory: \n",
    "            Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient (path=self.persist_directory)\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print (f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        Args:\n",
    "        documents: List of LangChain documents\n",
    "        embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        print(f\"Adding {len (documents)} documents to vector store...\")\n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        for i, (doc, embedding) in enumerate (zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "            # Add to collection\n",
    "            try:\n",
    "                self.collection.add(\n",
    "                    ids=ids,\n",
    "                    embeddings=embeddings_list,\n",
    "                    metadatas=metadatas,\n",
    "                    documents=documents_text\n",
    "                )\n",
    "                print (f\"Successfully added {len (documents)} documents to vector store\")\n",
    "                print (f\"Total documents in collection: {self.collection.count()}\")\n",
    "            except Exception as e:\n",
    "                print (f\"Error adding documents to vector store: {e}\")\n",
    "                raise\n",
    "\n",
    "vector_store = VectorStore()\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3612b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 131 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (131, 384)\n",
      "Adding 131 documents to vector store...\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 220\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 221\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 222\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 223\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 224\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 225\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 226\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 227\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 228\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 229\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 230\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 231\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 232\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 233\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 234\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 235\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 236\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 237\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 238\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 239\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 240\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 241\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 242\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 243\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 244\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 245\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 246\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 247\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 248\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 249\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 250\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 251\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 252\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 253\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 254\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 255\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 256\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 257\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 258\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 259\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 260\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 261\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 262\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 263\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 264\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 265\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 266\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 267\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 268\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 269\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 270\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 271\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 272\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 273\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 274\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 275\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 276\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 277\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 278\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 279\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 280\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 281\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 282\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 283\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 284\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 285\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 286\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 287\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 288\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 289\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 290\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 291\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 292\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 293\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 294\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 295\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 296\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 297\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 298\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 299\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 300\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 301\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 302\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 303\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 304\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 305\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 306\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 307\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 308\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 309\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 310\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 311\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 312\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 313\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 314\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 315\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 316\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 317\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 318\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 319\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 320\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 321\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 322\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 323\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 324\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 325\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 326\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 327\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 328\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 329\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 330\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 331\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 332\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 333\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 334\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 335\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 336\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 337\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 338\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 339\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 340\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 341\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 342\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 343\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 344\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 345\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 346\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 347\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 348\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 349\n",
      "Successfully added 131 documents to vector store\n",
      "Total documents in collection: 350\n"
     ]
    }
   ],
   "source": [
    "# chunks\n",
    "# ## Convert text to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "# texts\n",
    "\n",
    "## Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## Store in vector DB\n",
    "vector_store.add_documents(chunks, embeddings)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310aa5b",
   "metadata": {},
   "source": [
    "### Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e30633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x7ffa5043f490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "\n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            retrieved_docs = []\n",
    "            if results.get(\"documents\") and results[\"documents\"][0]:\n",
    "                documents = results[\"documents\"][0]\n",
    "                metadatas = results[\"metadatas\"][0]\n",
    "                distances = results[\"distances\"][0]\n",
    "                ids = results[\"ids\"][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    similarity_score = 1 - distance  # convert cosine distance → similarity\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"content\": document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            \"distance\": distance,\n",
    "                            \"rank\": i + 1\n",
    "                        })\n",
    "\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering).\")\n",
    "            else:\n",
    "                print(\"No documents found in results.\")\n",
    "\n",
    "            return retrieved_docs\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store, embedding_manager)\n",
    "rag_retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc89115d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Multi head attention'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents (after filtering).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_56fb41a8_20',\n",
       "  'content': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].',\n",
       "  'metadata': {'created': '2017',\n",
       "   'page': 4,\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'page_label': '5',\n",
       "   'language': 'en-US',\n",
       "   'date': '2017',\n",
       "   'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "   'type': 'Conference Proceedings',\n",
       "   'book': 'Advances in Neural Information Processing Systems 30',\n",
       "   'published': '2017',\n",
       "   'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       "   'publisher': 'Curran Associates, Inc.',\n",
       "   'creationdate': '',\n",
       "   'producer': 'PyPDF2',\n",
       "   'content_length': 949,\n",
       "   'firstpage': '5998',\n",
       "   'lastpage': '6008',\n",
       "   'total_pages': 11,\n",
       "   'source': '../data/pdf_files/attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creator': 'PyPDF',\n",
       "   'doc_index': 20,\n",
       "   'source_file': 'attention.pdf',\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'eventtype': 'Poster',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/'},\n",
       "  'similarity_score': 0.20299416780471802,\n",
       "  'distance': 0.797005832195282,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_7451c328_20',\n",
       "  'content': 'MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].',\n",
       "  'metadata': {'doc_index': 20,\n",
       "   'page_label': '5',\n",
       "   'creationdate': '',\n",
       "   'creator': 'PyPDF',\n",
       "   'firstpage': '5998',\n",
       "   'publisher': 'Curran Associates, Inc.',\n",
       "   'published': '2017',\n",
       "   'created': '2017',\n",
       "   'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       "   'eventtype': 'Poster',\n",
       "   'date': '2017',\n",
       "   'source': '../data/pdf_files/attention.pdf',\n",
       "   'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       "   'language': 'en-US',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'total_pages': 11,\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'book': 'Advances in Neural Information Processing Systems 30',\n",
       "   'page': 4,\n",
       "   'lastpage': '6008',\n",
       "   'producer': 'PyPDF2',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'type': 'Conference Proceedings',\n",
       "   'content_length': 949},\n",
       "  'similarity_score': 0.20299416780471802,\n",
       "  'distance': 0.797005832195282,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_dcf100bc_19',\n",
       "  'content': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'date': '2017',\n",
       "   'eventtype': 'Poster',\n",
       "   'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       "   'language': 'en-US',\n",
       "   'total_pages': 11,\n",
       "   'lastpage': '6008',\n",
       "   'content_length': 834,\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'publisher': 'Curran Associates, Inc.',\n",
       "   'firstpage': '5998',\n",
       "   'creationdate': '',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'producer': 'PyPDF2',\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'source': '../data/pdf_files/attention.pdf',\n",
       "   'creator': 'PyPDF',\n",
       "   'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       "   'page': 3,\n",
       "   'published': '2017',\n",
       "   'type': 'Conference Proceedings',\n",
       "   'doc_index': 19,\n",
       "   'book': 'Advances in Neural Information Processing Systems 30',\n",
       "   'created': '2017',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "   'page_label': '4'},\n",
       "  'similarity_score': 0.17939472198486328,\n",
       "  'distance': 0.8206052780151367,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_e3a6a25b_19',\n",
       "  'content': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'book': 'Advances in Neural Information Processing Systems 30',\n",
       "   'doc_index': 19,\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'creationdate': '',\n",
       "   'total_pages': 11,\n",
       "   'publisher': 'Curran Associates, Inc.',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       "   'producer': 'PyPDF2',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'published': '2017',\n",
       "   'created': '2017',\n",
       "   'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "   'page_label': '4',\n",
       "   'source': '../data/pdf_files/attention.pdf',\n",
       "   'eventtype': 'Poster',\n",
       "   'lastpage': '6008',\n",
       "   'creator': 'PyPDF',\n",
       "   'content_length': 834,\n",
       "   'source_file': 'attention.pdf',\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'type': 'Conference Proceedings',\n",
       "   'firstpage': '5998',\n",
       "   'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       "   'date': '2017',\n",
       "   'page': 3,\n",
       "   'language': 'en-US'},\n",
       "  'similarity_score': 0.17939472198486328,\n",
       "  'distance': 0.8206052780151367,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_c2cb3248_30',\n",
       "  'content': 'convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece',\n",
       "  'metadata': {'page_label': '7',\n",
       "   'published': '2017',\n",
       "   'eventtype': 'Poster',\n",
       "   'subject': 'Neural Information Processing Systems http://nips.cc/',\n",
       "   'doc_index': 30,\n",
       "   'page': 6,\n",
       "   'creator': 'PyPDF',\n",
       "   'creationdate': '',\n",
       "   'type': 'Conference Proceedings',\n",
       "   'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin',\n",
       "   'content_length': 970,\n",
       "   'producer': 'PyPDF2',\n",
       "   'created': '2017',\n",
       "   'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.',\n",
       "   'title': 'Attention is All you Need',\n",
       "   'book': 'Advances in Neural Information Processing Systems 30',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett',\n",
       "   'file_type': 'pdf',\n",
       "   'publisher': 'Curran Associates, Inc.',\n",
       "   'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)',\n",
       "   'source': '../data/pdf_files/attention.pdf',\n",
       "   'language': 'en-US',\n",
       "   'date': '2017',\n",
       "   'total_pages': 11,\n",
       "   'moddate': '2018-02-12T21:22:10-08:00',\n",
       "   'lastpage': '6008',\n",
       "   'firstpage': '5998'},\n",
       "  'similarity_score': 0.050618648529052734,\n",
       "  'distance': 0.9493813514709473,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag_retriever.retrieve(\"Transformer architecture\")\n",
    "# print(vector_store.collection.count())\n",
    "rag_retriever.retrieve(\"Multi head attention\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249c6b6",
   "metadata": {},
   "source": [
    "### Integrating VectorDB Context with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bff610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import SystemMessage, HumanMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"openai/gpt-oss-20b\", temperature=0.1, max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple (query, retriever, llm, top_k=3):\n",
    "    ## retriever the context\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "                    Context: {context}\n",
    "                    Question: {query}\n",
    "                    Answer:\n",
    "                \"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "# def rag_simple(query, retriever, llm, top_k=3):\n",
    "#     # 1. Retrieve relevant context\n",
    "#     results = retriever.retrieve(query, top_k=top_k)\n",
    "#     context = \"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "\n",
    "#     if not context:\n",
    "#         return \"No relevant context found to answer the question.\"\n",
    "\n",
    "#     # 2. Build the prompt\n",
    "#     prompt = f\"\"\"\n",
    "#         You are an AI assistant answering based on the provided context.\n",
    "\n",
    "#         Context:\n",
    "#         {context}\n",
    "\n",
    "#         Question: {query}\n",
    "\n",
    "#         Answer concisely and clearly:\n",
    "#     \"\"\"\n",
    "\n",
    "#     # 3. Generate response using Groq LLM\n",
    "#     response = llm.invoke([HumanMessage(content=prompt)])\n",
    "#     return response.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18da7ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 0 documents (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No relevant context found to answer the question.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = rag_simple(\"What is attention mechanism\", rag_retriever, llm)\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b8656",
   "metadata": {},
   "source": [
    "### Advances RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5bf625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Applications of Attention in our Model'\n",
      "Top K: 5, Score threshold: 0.1\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  **Applications of Attention in Our Model**\n",
      "\n",
      "- **Parallel Multi‑Head Processing**  \n",
      "  - Queries, keys, and values are linearly projected into \\(d_k\\), \\(d_k\\), and \\(d_v\\) dimensions for each head.  \n",
      "  - Each head computes attention independently, enabling the model to capture diverse patterns in parallel.\n",
      "\n",
      "- **Joint Subspace Attention**  \n",
      "  - By attending to different representation subspaces simultaneously, the model can focus on multiple aspects of the input (e.g., syntax, semantics) at the same time.\n",
      "\n",
      "- **Concatenation & Final Projection**  \n",
      "  - The \\(d_v\\)-dimensional outputs from all heads are concatenated and projected back to the original dimension, producing a rich, fused representation.\n",
      "\n",
      "- **Enhanced Expressiveness**  \n",
      "  - Multi‑head attention mitigates the averaging effect of a single head, allowing the network to learn more nuanced relationships across positions.\n",
      "\n",
      "- **Scalable Context Capture**  \n",
      "  - The dot‑product attention mechanism scales with the dimensionality \\(d_k\\), enabling efficient modeling of long‑range dependencies while keeping computational cost manageable.\n",
      "Sources:  [{'source': 'attention.pdf', 'page': 3, 'score': 0.12982487678527832, 'preview': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensio...'}, {'source': 'attention.pdf', 'page': 3, 'score': 0.12982487678527832, 'preview': 'we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensio...'}]\n",
      "Confidence:  0.12982487678527832\n",
      "Context Preview:  we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensio\n"
     ]
    }
   ],
   "source": [
    "def rag_advanced (query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve (query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc [ 'content'] for doc in results ])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'] [:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\n",
    "                    Context: {context}\n",
    "                    Question: {query}\n",
    "                    Answer:\n",
    "    \"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "result = rag_advanced(\"Applications of Attention in our Model\", rag_retriever, llm, top_k=5, min_score=0.1, return_context=True)\n",
    "print(\"Answer: \", result['answer'])\n",
    "print(\"Sources: \", result['sources'])\n",
    "print(\"Confidence: \", result['confidence'])\n",
    "print(\"Context Preview: \", result['context'][:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0439617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90a1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
