{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d140ee19",
   "metadata": {},
   "source": [
    "### Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bf04ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'author': 'me', 'pages': 1, 'source': 'sample.txt'}, page_content='This is a sample text content in build RAG.')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document = Document(\n",
    "    page_content=\"This is a sample text content in build RAG.\",\n",
    "    metadata={\n",
    "        \"author\": \"me\",\n",
    "        \"pages\": 1,\n",
    "        \"source\": \"sample.txt\"\n",
    "    }\n",
    ")\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ab43e",
   "metadata": {},
   "source": [
    "### Text Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b88ef27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/internet.txt'}, page_content='The internet began as a research project funded by the U.S. Department of Defense in the 1960s. \\nKnown as ARPANET, it connected several universities to share information digitally. \\nBy the 1990s, the World Wide Web was developed by Tim Berners-Lee, making it possible \\nfor anyone to publish and access content through browsers. Today, the internet is a global \\nsystem linking billions of devices and enabling services like email, cloud computing, and social media.\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/internet.txt\", encoding=\"utf-8\")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d5224",
   "metadata": {},
   "source": [
    "### Directory Loader - For Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acc6e496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/renewable.txt'}, page_content='Renewable energy comes from sources that are naturally replenished, such as sunlight, wind, rain, \\nand geothermal heat. Solar power uses photovoltaic cells to convert sunlight into electricity, while \\nwind turbines transform wind energy into power. These sources are increasingly important as the world \\nmoves away from fossil fuels to combat climate change. However, challenges remain in energy \\nstorage and grid integration.\\n'),\n",
       " Document(metadata={'source': '../data/text_files/internet.txt'}, page_content='The internet began as a research project funded by the U.S. Department of Defense in the 1960s. \\nKnown as ARPANET, it connected several universities to share information digitally. \\nBy the 1990s, the World Wide Web was developed by Tim Berners-Lee, making it possible \\nfor anyone to publish and access content through browsers. Today, the internet is a global \\nsystem linking billions of devices and enabling services like email, cloud computing, and social media.\\n')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls= TextLoader,\n",
    "    show_progress=False,\n",
    ")\n",
    "\n",
    "docs = dir_loader.load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abff60",
   "metadata": {},
   "source": [
    "### Directory Loader - For PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f6f1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.26', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-09-19T16:36:04+00:00', 'source': '../data/pdf_files/Badesab Mohammed Mohassin Hussain-Resume.pdf', 'file_path': '../data/pdf_files/Badesab Mohammed Mohassin Hussain-Resume.pdf', 'total_pages': 1, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-19T16:36:04+00:00', 'trapped': '', 'modDate': 'D:20250919163604Z', 'creationDate': 'D:20250919163604Z', 'page': 0}, page_content='Badesab Mohammed Mohassin Hussain\\n+91 9676940969 | mohassinhussain22@gmail.com | linkedin.com/in/mohassinhussain | github.com/MohassinHussain\\nportfolio\\nEducation\\nCMR College of Engineering & Technology\\nKandlakoya, Telangana\\nBachelor of Technology on AI & ML - CGPA: 8.8\\nNov. 2022 – Jul 2026\\nExperience\\nFreelance Frontend Developer & SEO Specialist\\nJan 2025 – Mar 2025\\nDental Clinic Website Project\\n• Led frontend development, SEO optimization, and product management for a dental clinic website.\\n• Implemented SEO strategies that increased patient visits by 20%+, boosting clinic revenue.\\n• Enhanced user experience (UI/UX), resulting in a 35% improvement in website engagement metrics.\\nProjects\\nbFinder | React Native, Redux, Firebase, Clerk Auth\\n• Developed an application to help users find suitable locations for business.\\n• Built-in AI analyzer for predicting the business success rate in an area.\\n• Demo: Link\\nRessa | React Native(Expo), TF-IDF cosine similarity, Flask\\n• Built a productive mobile application to store and schedule various learning resources (5+ users).\\n• Optimized model for generating relevant resources by surfing the web on behalf of the user.\\n• APK: Link — GitHub: Link\\nFYLz | React.js, MongoDB, Node.js, Express.js\\n• Developed a secure and efficient file-sharing platform with real-time state management using React.js.\\n• Ensured secure and temporary file storage with automatic deletion; used Multer for handling multipart data.\\n• Live Demo: Link\\ninApp-Share (iAS) | React.js, Tailwind CSS, MongoDB, Firebase Auth, Node.js\\n• Built a feature for a YouTube-like video streaming platform without relying on external platforms for sharing\\ncontent.\\n• Demo: Link\\nFake Logo Detection | Python, CNN, Deep Learning, TKinter\\n• Optimized image detection and classification using CNN for image processing.\\nOthers | LLMs, HuggingFace, Docker model runner, Personal File Analyzer, Personal Researcher agent using n8n\\n• Practiced various AI and automation tools using docker including LLMs and Models.\\nTechnical Skills\\nData Structures & Algorithms (LeetCode(150+), CodeForces)\\nLanguages: Java, Python, C/C++, JavaScript & TS, SQL\\nLibraries/Frameworks: HTML, CSS, React.js, React Native, Next.js, Tailwind CSS, Node.js, Express.js, Django\\nBlockchain Technologies: Solidity, Ether.js, IPFS, Distributed Networks\\nDeveloper Tools/Platforms: Git, Docker, HuggingFace, Firebase, VS Code\\nDatabases: MongoDB, MySQL, Firebase\\nOthers: LLMs, n8n, ollama, open-source\\nCoursework: Operating Systems, Computer Networks, Database Management Systems, Software Engineering\\nAchievements\\nOracle Certified Professional: OCI Certified 2025 Gen AI Professional.\\nCisco: Certified in Fundamentals of Cybersecurity Student Level Credential.\\nGSSoC Contributor: Contributed to 2 projects under GSSoC extd. 2024.\\nMentorship: Mentored more than 5 teams to complete their projects.\\nEvent Hosting: Hosted a Technical Event, BugRack. Got more than 30 participants.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'file_path': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T08:32:37+00:00', 'trapped': '', 'modDate': \"D:20250915083237+00'00'\", 'creationDate': \"D:20250915083237+00'00'\", 'page': 0}, page_content=\"Hello! How can I assist you today?\\nWhen resetting Windows 11 in a dual-boot system with Linux, the Windows reset process may\\noverwrite the bootloader, which can remove or interfere with the GRUB bootloader that manages\\nLinux boot. To avoid losing GRUB when resetting Windows 11, consider these key points:\\nIn summary, resetting Windows 11 alone tends to remove GRUB, but if you are prepared to\\nrestore GRUB using a Linux Live USB afterward, the dual boot can be recovered without data\\nloss on Linux. Carefully manage BIOS boot priorities and be ready to reinstall GRUB if needed.\\nIf preferred, detailed steps for restoring GRUB after a Windows reset or installation are available,\\nincluding mounting the Linux partition and running sudo grub-install commands from a Live\\nenvironment. Would you like step-by-step instructions for restoring GRUB after resetting\\nWindows?\\n⁂\\nHi\\nI have windows 11 and linux I want to reset the\\nwindows and I dont want the grub to be removed\\nBefore the reset, ensure you have a Linux Live USB or rescue media ready, so you can\\nrestore GRUB if it's overwritten. Windows reset typically replaces GRUB with the Windows\\nbootloader, removing the Linux boot option.\\nAfter resetting Windows, if GRUB is missing, it can be restored using a Linux Live\\nenvironment by reinstalling GRUB with commands like grub-install and update-grub\\ntargeted at your Linux partition.\\nAlternatively, checking BIOS/UEFI boot order before and after the reset to ensure the Linux\\nbootloader remains prioritized can prevent Windows from automatically booting and\\nbypassing GRUB.\\nDisabling Windows fast startup before the reset reduces potential interference with the\\nbootloader.\\nSome users suggest setting the Linux disk as primary boot device in BIOS/UEFI, so the\\nGRUB loader controls the boot process, even if Windows modifies its own bootloader.\\n[1]\\n[2] [3]\\n[4] [1]\\n1. https://pq.hosting/en/help/restoring-grub-after-installing-windows-10\\n2. https://www.reddit.com/r/linux4noobs/comments/w6qtrx/will_the_windows_11_update_break_dual_boot/\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m127', 'creator': 'Chromium', 'creationdate': '2025-09-15T08:32:37+00:00', 'source': '../data/pdf_files/pplx_search.pdf', 'file_path': '../data/pdf_files/pplx_search.pdf', 'total_pages': 2, 'format': 'PDF 1.4', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-09-15T08:32:37+00:00', 'trapped': '', 'modDate': \"D:20250915083237+00'00'\", 'creationDate': \"D:20250915083237+00'00'\", 'page': 1}, page_content='3. https://discussion.fedoraproject.org/t/repair-reinstall-grub-after-windows-11-update-dual-boot-fedora\\n-f39/113155\\n4. https://www.cybrary.it/blog/restoring-grub-installing-windows\\n5. https://learn.microsoft.com/en-us/answers/questions/4166685/(solved)-accidentally-deleted-windows-1\\n1-microsoft?forum=windows-all\\n6. https://www.reddit.com/r/linuxmint/comments/1fmn07i/reinstalling_windows_wiped_my_grub_but_now_i\\n_cant/\\n7. https://learn.microsoft.com/en-us/answers/questions/3957734/why-isnt-my-windows-booting-after-du\\nal-booting-lin\\n8. https://forum.manjaro.org/t/remove-grub-and-revert-to-windows-bootloader/103716\\n9. https://www.youtube.com/watch?v=PVxDttc2z-I\\n10. https://learn.microsoft.com/en-us/answers/questions/4115492/i-deleted-a-partition-from-windows-11-th\\nat-had-lin\\n11. https://learn.microsoft.com/en-us/answers/questions/2796971/how-to-restore-windows-bootloader-fro\\nm-grub?forum=windows-all\\n12. https://wiki.archlinux.org/title/Dual_boot_with_Windows\\n13. https://discourse.ubuntu.com/t/grub-install-issues-on-laptop-with-win11/52702\\n14. https://www.reddit.com/r/linuxmint/comments/1hawohz/how_to_restore_window_11_bootloader_no_longe\\nr/\\n15. https://www.youtube.com/watch?v=I_1fk15QQ_M\\n16. https://www.youtube.com/watch?v=KWVte9WGxGE\\n17. https://forum.endeavouros.com/t/no-grub-after-windows-10-install-how-to-recover/45977\\n18. https://www.onlogic.com/blog/how-to-dual-boot-windows-11-and-linux/'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 1}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 4}, page_content='MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 6}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\nResidual Dropout\\nWe apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [15]\\n23.75\\nDeep-Att + PosUnk [32]\\n39.2\\n1.0 · 1020\\nGNMT + RL [31]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [8]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [26]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [32]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [31]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [8]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.0\\n2.3 · 1019\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 9}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': '', 'creationdate': '', 'source': '../data/pdf_files/attention.pdf', 'file_path': '../data/pdf_files/attention.pdf', 'total_pages': 11, 'format': 'PDF 1.3', 'title': 'Attention is All you Need', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'keywords': '', 'moddate': '2018-02-12T21:22:10-08:00', 'trapped': '', 'modDate': \"D:20180212212210-08'00'\", 'creationDate': '', 'page': 10}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 0}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2347] \\nEXPLORING TECH FOUNDATIONS: DSA, OOP, DBMS, OS IN  \\nCOMPUTER SCIENCE \\nSushmita C. Hubli*1 \\n*1Department Of Electronics And Telecommunication, Pune Institute Of Computer Technology,  \\nPune, India. \\nDOI : https://www.doi.org/10.56726/IRJMETS47559 \\nABSTRACT \\nThis technical paper embarks on a comprehensive exploration of the foundational pillars in computer science, \\nnamely Data Structures and Algorithms (DSA), Object-Oriented Programming (OOP), Database Management \\nSystems (DBMS), and Operating Systems (OS). Delving beyond theoretical boundaries, our journey traverses \\nthe practical landscapes where these domains intersect, illustrating their collective impact on the intricate \\ntapestry of modern computing. From the intricate choreography of data manipulation in DSA to the elegant \\nmodularity of OOP, the strategic data orchestration in DBMS, and the orchestration of hardware resources in \\nOS, this paper navigates the intricate interplay that forms the backbone of technological innovation. By \\nunraveling these key subjects, this work endeavors to equip both novice learners and seasoned professionals \\nwith a nuanced understanding of the bedrock principles that propel the field of computer science forward into \\nan era of unprecedented possibilities.  \\nKeywords: Data Structures And Algorithms, Object-Oriented Programming, Database Management Systems, \\nAnd Operating Systems. \\nI. \\nINTRODUCTION \\nIn the dynamic and ever-evolving landscape of computer science, the quest for knowledge transcends the \\nboundaries of theoretical abstraction. This technical paper embarks on a grand intellectual journey, venturing \\ninto the core tenets that underpin the very fabric of modern computing. Data Structures and  \\nAlgorithms (DSA), Object-Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) stand as the cornerstones of this exploration, not merely as disparate subjects but as \\ninterconnected realms that collectively propel the domain of computer science into new frontiers.  \\nThe accelerated pace of technological advancement renders a profound understanding of these fundamental \\nsubjects not just advantageous, but essential. As the architects of the digital age, computer scientists must \\nnavigate the intricate interplay between DSA, OOP, DBMS, and OS to craft solutions that transcend mere \\nfunctionality, embracing elegance and efficiency. It is within this nexus that the boundaries between theory and \\npractice blur, and the essence of true innovation is realized.  \\nOur journey into the heart of each domain is not a mere academic exercise but an immersion into the practical \\nrealms where lines of code breathe life into algorithms, where data transforms into actionable insights, and \\nwhere operating systems choreograph the symphony of hardware resources. From themeticulous \\nchoreography of algorithms in DSA to the elegant encapsulation of OOP principles, the strategic orchestration of \\ndatabases in DBMS, and the intricate management of resources in OS, our exploration seeks to bridge the gap \\nbetween foundational principles and real-world applications. This endeavor is more than a mere academic \\npursuit—it is an invitation to comprehend the language of technology, to decipher the code that powers \\ninnovation, and to grasp the architecture that sustains the digital age. Through this exploration, both novice \\nlearners and seasoned professionals are invited to navigate the intricate crossroads of DSA, OOP, DBMS, and OS, \\nemerging not just with knowledge but with a holistic comprehension of the structures that underlie every byte \\nof our digital existence. As we embark on this intellectual odyssey, the goal is clear: to empower minds with the \\nwisdom required to not just navigate the currents of contemporary computing but to shape its course and \\ncontribute to the ever-expanding frontier of technological possibilities. In the next sections, we will unravel the \\nintricacies of DSA, OOP, DBMS, and OS, forging a profound connection between theory and practice in the realm \\nof computer science.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 1}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2348] \\nII. \\nFUNDAMENTALS OVERVIEW \\nDSA \\nData Structures and Algorithms (DSA) refer to the foundational concepts in computer science for organizing \\nand processing data efficiently. DSA involves the study of structures like arrays, linked lists, stacks, queues, \\ntrees, and graphs, each serving specific data storage and retrieval purposes. Algorithms are step-by-step \\nprocedures or sets of rules designed to solve computational problems. DSA is fundamental for optimizing data \\nmanipulation and solving complex computational tasks. It is applied in various domains, from software \\ndevelopment to artificial intelligence. DSA enables the creation of efficient search and sorting mechanisms, \\ncontributing to the design of high-performance software systems. Mastery of DSA is crucial for algorithmic \\nproblem-solving and optimizing resource utilization. It provides a framework for understanding the trade-offs \\nbetween different data structures and algorithms in solving computational challenges. DSA is an integral part of \\ncomputer science education, empowering students and professionals to build scalable and effective software \\nsolutions.  \\nLet's delve into the essence of DSA, unraveling the intricacies of various data structures and algorithms that \\nform the backbone of computational thinking. \\n1. Arrays: \\nArrays are fundamental data structures in computer science, representing a contiguous block of memory where \\nelements of the same data type are stored in a linear fashion. They serve as versatile and efficient containers, \\nproviding a systematic way to organize and access data through a numerical index. The defining characteristic \\nof arrays is their fixed size, established during declaration, making them well-suited for scenarios where the \\nquantity of elements is known in advance. Each element in the array occupies a specific memory location, and \\nits position is determined by its index, starting from zero. This indexing system allows for direct and constant-\\ntime access to elements, enhancing the efficiency of data retrieval. Arrays find extensive application in various \\ndomains, from simple data storage to complex algorithms, offering a foundational building block for the \\nimplementation of more intricate data structures and computational processes. Despite their fixed size \\nlimitation, arrays' simplicity, speed, and memory efficiency make them indispensable in a multitude of \\nprogramming scenarios.  \\nArrays are widely used in various real-world applications. They play a crucial role in image processing for \\nefficient pixel representation, store sensor data in IoT devices, facilitate audio signal processing, organize data \\nin database systems, enable genomic data analysis in bioinformatics, support graphical user interfaces, \\ncontribute to financial modeling, and are essential for mathematical computations using matrices. Arrays \\nprovide a structured and efficient way to organize and access data, making them a foundational data structure \\nin computer science with broad practical implications.  \\nReal-World Application: Student Grading System  \\nIn educational institutions, arrays are commonly used to store and manage student grades efficiently. Imagine a \\nscenario where each student's grades for different subjects are organized using arrays. For instance, an array \\ncould represent the grades for a specific subject, with each element corresponding to a student. The array \\nallows for systematic storage and quick retrieval of grades based on student indices. This structure facilitates \\nstraightforward calculations such as computing averages, identifying highest and lowest scores, and generating \\nreports. The use of arrays in this context streamlines the management of large datasets, providing a scalable \\nand organized solution for tracking and analyzing student performance in diverse academic subjects. \\n \\nFig. 1: BASIC ARRAY STRUCTURE\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 2}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2349] \\n2. Linked Lists: \\nA linked list is a dynamic data structure that consists of a collection of nodes, each containing data and a \\nreference or link to the next node in the sequence. Unlike arrays, linked lists do not require contiguous memory \\nallocation, allowing for flexible and efficient insertion and deletion operations. The first node, known as the \\nhead, serves as the starting point, and the last node typically points to null, indicating the end of the list. Linked \\nlists come in various forms, including singly linked lists where nodes have a reference to the next node, and \\ndoubly linked lists where nodes have references to both the next and previous nodes. This structure allows for \\nsequential traversal, insertion, and removal of elements, making linked lists well-suited for dynamic scenarios \\nwhere the size of the data set may change frequently. While random access to elements is slower compared to \\narrays, linked lists excel in scenarios where dynamic memory allocation and efficient insertion or deletion \\noperations are crucial, such as in applications involving real-time data updates or task scheduling.  \\nLinked lists find practical application in scenarios that require dynamic data management and frequent \\ninsertions or deletions. One notable real-world application is in task scheduling and management systems. \\nLinked lists enable the efficient representation of tasks, where each node corresponds to a specific task, and the \\nlinks between nodes facilitate sequential processing. As tasks are added or completed, the linked list can be \\ndynamically adjusted, ensuring a flexible and responsive task scheduling mechanism. This adaptability makes \\nlinked lists valuable in real-time environments, such as operating systems or project management tools, where \\ntask priorities and order may change dynamically, requiring a data structure that can easily accommodate such \\nupdates.  \\nReal-World Application: Music Playlist  \\nIn the design of music playlist applications, linked lists can be employed to create dynamic and flexible playlists. \\nEach song in the playlist can be represented as a node in the linked list, where each node contains information \\nabout the song (such as title, artist, and duration) and a reference to the next song in the playlist. This linked list \\nstructure enables easy rearrangement of songs, insertion of new songs, and removal of existing ones. When a \\nuser adds a new song to the playlist, it becomes a new node linked to the previous song, creating a seamless \\nflow. Similarly, removing a song or rearranging the order involves updating the references between nodes. This \\ndynamic and adaptable structure makes linked lists an ideal choice for managing playlists, offering a user-\\nfriendly and efficient way to organize and enjoy music in applications like music streaming services. \\n \\nFig. 2: BASIC STRUCTURE OF LINKED LIST \\n3. Stacks: \\nA stack is a fundamental data structure that follows the Last In, First Out (LIFO) principle, designed to manage a \\ncollection of elements with two primary operations: push, which adds an element to the top of the stack, and \\npop, which removes the topmost element. The stack operates as a dynamic, ordered set where elements are \\nstacked on one another, resembling a vertical structure. The top of the stack is the most recently added \\nelement, while the bottom represents the initial element. Stacks find widespread application in various \\ncomputing scenarios, such as managing function calls during program execution, undo mechanisms in software \\napplications, and parsing expressions in compilers. The LIFO structure allows for efficient memory \\nmanagement as elements are added and removed from the top, and the simplicity of these operations makes \\nstacks essential for maintaining order in algorithms, managing recursive processes, and ensuring organized \\ndata storage and retrieval.  \\nStacks are applied in various real-world scenarios due to their Last In, First Out (LIFO) structure. One notable \\napplication is in undo mechanisms of software applications. When users perform actions like typing or \\nformatting, each action is pushed onto a stack. The most recent action is always at the top. If users decide to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 3}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2350] \\nundo an operation, the system pops the top element off the stack, effectively reversing the last action. This \\nimplementation provides a straightforward and efficient way to manage a history of user actions, offering a \\nseamless and intuitive undo functionality in applications ranging from text editors to graphic design software.  \\nReal-World Application: Web Browser Navigation  \\nWhen you navigate through web pages using a browser, the stack data structure is employed to manage the \\nhistory of visited pages and enable the \"Back\" and \"Forward\" functionalities. Each time you visit a new page, it \\nis pushed onto the stack. If you click the \"Back\" button, the browser pops the top page from the stack, \\neffectively taking you back to the previously visited page. Conversely, if you click the \"Forward\" button, the \\nbrowser pushes the next page onto the stack, allowing you to move forward through your browsing history. \\nThis stack-based approach provides a simple and efficient way to track and navigate through the sequence of \\nweb pages you have visited, enhancing the overall user experience in web browsing applications. \\n \\nFig. 3: BASIC STRUCTURE OF STACK \\n4. Queues: \\nA queue is a fundamental data structure that adheres to the First In, First Out (FIFO) principle, serving as an \\nordered collection of elements where insertion occurs at the rear, and removal takes place at the front. This \\ndynamic structure operates like a real-world queue, where entities join at the back and are served or processed \\nfrom the front. Envisioned as a linear arrangement, each element in the queue, often referred to as a \"node,\" \\ncontains data and a reference to the next node in the sequence. Queues find extensive application in scenarios \\nwhere tasks or data must be processed in the order they are received, such as print job scheduling in operating \\nsystems, managing tasks in asynchronous systems, or modeling processes in computer networks. The \\nsimplicity and efficiency of the FIFO mechanism make queues instrumental in scenarios demanding orderly and \\nsequential data processing, ensuring that the first element enqueued is the first to be dequeued, maintaining a \\nstructured flow of operations. Queues are widely employed in scenarios requiring orderly and sequential data \\nprocessing. One prominent real-world application is in print job scheduling within operating systems. Print \\njobs are enqueued as they are submitted to the printer queue, and they are processed in the order they are \\nreceived. This First In, First Out (FIFO) approach ensures fairness in task execution, preventing resource \\ncontention and providing an organized mechanism for handling print requests. Queues are integral in managing \\ntasks in a sequential manner, making them valuable in various systems where tasks need to be processed in the \\norder they arrive, contributing to efficient and systematic data flow.  \\nReal-World Application: Customer Support Chat Queue  \\nIn online customer support systems, a queue is often employed to manage incoming customer queries in a fair \\nand organized manner. When customers initiate chat sessions for assistance, their requests join a queue, \\nforming a line based on the order of arrival. The customer service representatives address inquiries in a First \\nIn, First Out (FIFO) fashion. This ensures that the earliest customer requests are attended to first, maintaining a \\nsense of fairness and timely responsiveness. The queue structure allows for a systematic approach to handling \\ncustomer inquiries, preventing bottlenecks, and providing an efficient and orderly way to manage the flow of \\ncustomer support interactions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 4}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2351] \\n \\nFig. 4: BASIC STRUCTURE OF QUEUE \\n5. Trees: \\nA tree is a hierarchical and widely used data structure in computer science that represents a collection of \\nelements organized in a branching structure. Comprising nodes connected by edges, a tree consists of a root \\nnode serving as the topmost element, and each node can have zero or more child nodes. Nodes in a tree are \\ninterconnected in a way that no circular paths exist, defining a directed acyclic graph. Nodes that share a \\ncommon parent are considered siblings, and those stemming from the same parent are referred to as subtrees. \\nTrees are characterized by their versatility and efficiency in representing hierarchical relationships, making \\nthem fundamental in various domains. They find applications in file systems where directories and files are \\norganized, database indexing for efficient data retrieval, search algorithms like binary search, and hierarchical \\ndata representation. With types such as binary trees, AVL trees, and Btrees, the hierarchical structure of trees \\nserves as a powerful paradigm for organizing and navigating complex relationships in diverse computational \\nscenarios.  \\nTrees are applied in various real-world scenarios due to their hierarchical structure. One notable application is \\nin file systems, where trees represent directories and files. The root node signifies the main directory, with \\nbranches extending to subdirectories and leaves representing individual files. This hierarchical organization \\nfacilitates efficient storage, retrieval, and navigation of files, exemplifying how trees enhance the structuring of \\ncomplex relationships. Trees are also pivotal in database indexing, optimizing search algorithms, and \\nrepresenting hierarchical relationships in applications such as organizational charts and network routing. Their \\nversatility and efficiency make trees a foundational data structure for managing and structuring information in \\ndiverse computational environments.  \\nReal-World Application: Company Organizational Chart  \\nConsider a large company with multiple departments, teams, and hierarchical levels. The organizational \\nstructure of such a company can be effectively represented using a tree. The root node would symbolize the \\nCEO or the top management, and each subsequent level would represent different management tiers, \\ndepartments, teams, and individual employees. Nodes branching from a higher-level node represent the \\nreporting structure, where employees report to their immediate superiors. This tree structure not only mirrors \\nthe organizational hierarchy but also simplifies tasks such as finding reporting relationships, understanding \\ndepartmental structures, and facilitating efficient communication within the company. The use of a tree in this \\ncontext provides a clear and visual representation of the company's organizational framework, aiding in \\ndecision-making, communication, and overall management. \\n \\nFig. 5: BASIC STRUCTURE OF TREE \\n6. Graphs: \\nA graph is a versatile and fundamental data structure in computer science, consisting of a set of nodes (vertices) \\ninterconnected by edges. These edges represent relationships or connections between nodes, and they can be\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 5}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2352] \\neither directed or undirected. Graphs can take various forms, including directed graphs (digraphs), where \\nedges have a specific direction, and undirected graphs, where edges have no direction.  \\nNodes in a graph may also have weights or labels, adding additional information to the relationships.  \\nGraphs find broad applications in modeling complex relationships and networks, ranging from social networks \\nand transportation systems to computer networks and project management. Graph algorithms, such as \\nDijkstra's algorithm and breadth-first search, are essential tools for analyzing and solving problems in areas \\nlike route optimization, network flow, and recommendation systems. The flexibility and adaptability of graphs \\nmake them a powerful representation for capturing and analyzing intricate relationships in diverse \\ncomputational domains.  \\nGraphs are widely employed in various real-world applications due to their ability to model complex \\nrelationships. One notable application is in social networks, where individuals are represented as nodes, and \\nconnections between them as edges. Graphs enable the analysis of social interactions, identification of \\ninfluential nodes, and the prediction of network behavior. They also play a crucial role in logistics and \\ntransportation systems, where nodes represent locations and edges denote routes. Graph algorithms are \\nutilized for optimizing transportation routes, managing network flow, and enhancing efficiency in diverse \\nscenarios, from package delivery to urban planning. The adaptability of graphs makes them a versatile tool for \\ncapturing and understanding intricate relationships in dynamic systems.  \\nReal-World Application: Social Network Analysis  \\nConsider a social network, such as Facebook or LinkedIn, where individuals are represented as nodes and \\nrelationships between them as edges. This network can be modeled as a graph, with each person being a node, \\nand connections (friendships or professional relationships) forming the edges. Graph algorithms can then be \\napplied to analyze the structure of the social network, identify key influencers, predict connections, and \\nunderstand the overall connectivity patterns. Social network analysis using graphs has applications in various \\nfields, from targeted advertising and content recommendation to understanding the spread of information and \\ninfluence within online communities. The graph representation provides a powerful tool for gaining insights \\ninto the dynamics and relationships within complex social systems. \\n \\nFig. 6: BASIC STRUCTURE OF GRAPH \\n7. Hash Tables: \\nA hashtable, or hash map, is a fundamental data structure that facilitates efficient data retrieval by associating \\nkeys with corresponding values through a process called hashing. It employs a hash function to convert keys \\ninto indices, where the associated values are stored in an array-like structure called a bucket. The key feature of \\na hashtable is its ability to provide constant-time average-case complexity for common operations, such as \\ninsertion, retrieval, and deletion, by minimizing collisions—situations where multiple keys hash to the same \\nindex. Collision resolution methods, such as chaining or open addressing, ensure that each bucket can store \\nmultiple key-value pairs. Hashtables find wide application in diverse computing scenarios, including database \\nindexing, symbol tables in compilers, and spell checkers, where the efficient retrieval of data based on keys is \\ncrucial for optimizing algorithmic performance. The adaptability and efficiency of hashtables make them \\nintegral in addressing challenges related to data access and retrieval in various computational domains.  \\nHashtables are extensively applied in various real-world scenarios, notably in database systems for efficient \\ndata retrieval. In this context, keys, representing unique identifiers or attributes, are hashed using a hash \\nfunction, and the resulting indices point to corresponding values stored in the hashtable. This ensures swift \\naccess to specific data entries, significantly reducing retrieval time compared to linear search methods. \\nHashtables play a pivotal role in optimizing the performance of database systems, making them indispensable\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 6}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2353] \\nfor tasks such as quick data lookup, indexing, and retrieval in applications ranging from search engines to \\nfinancial systems, where speed and efficiency in accessing and managing data are critical.  \\nReal-World Application: Spell Checking in Text Editors  \\nHashtables are commonly employed in spell checkers within text editors to ensure efficient and rapid word \\nlookup. In this application, each word in a dictionary is associated with a unique key using a hash function. The \\nhashtable then stores these keys along with their corresponding words. During spell checking, when a user \\ninputs a word, the spell checker hashes the word to find its corresponding key and quickly checks if it exists in \\nthe hashtable. This process allows for near-instantaneous verification of the word's correctness. Hashtables are \\npivotal in this context because they provide constant-time averagecase complexity for key-based operations, \\nensuring a swift and responsive spell-checking process, which is crucial for enhancing the user experience in \\ntext editing applications. \\n8. Heaps: \\nA heap is a specialized tree-based data structure that satisfies the heap property, which distinguishes it as \\neither a max-heap or a min-heap. In a max-heap, for any given node, the value of the node is greater than or \\nequal to the values of its children, ensuring that the maximum element is at the root. Conversely, in a min-heap, \\nthe value of each node is less than or equal to the values of its children, making the minimum element the root. \\nHeaps are typically implemented as binary trees, where the relationship between parent and child nodes is \\nmaintained by the heap property. The key feature of heaps lies in their ability to provide efficient access to the \\nmaximum or minimum element, making them valuable in priority queue implementations. Heaps find \\napplications in various algorithms, including heap sort, Dijkstra's algorithm for shortest paths, and in-memory \\nsorting operations, where the logarithmic height of the heap ensures fast retrieval and manipulation of extreme \\nvalues. The versatility and performance characteristics of heaps make them a fundamental data structure in \\nalgorithmic design and optimization.  \\nOne notable application of heaps is in priority queues, where elements are assigned priorities, and the highest \\n(or lowest) priority can be efficiently accessed and removed. This is crucial in scenarios such as task scheduling, \\nemergency room triage, and network routing, where tasks, patients, or data packets need to be processed based \\non priority levels. Heaps enable quick identification of the highest-priority element, ensuring that critical \\noperations are performed first. This versatility makes heaps integral in optimizing resource allocation and task \\nexecution in various real-world applications where prioritization is a key factor.  \\nReal-World Application: Emergency Room Triage  \\nConsider an emergency room (ER) scenario where patients arrive with varying degrees of medical urgency. A \\nheap-based priority queue can be employed to manage the triage process efficiently. Each patient is assigned a \\npriority based on the severity of their condition, with the highest priority given to the most critical cases. As \\nnew patients arrive, their information, including the severity of their condition, is added to the priority queue \\nimplemented as a max-heap. The patient with the highest priority (most critical condition) is quickly identified \\nand attended to by the medical staff. This ensures that critical cases are addressed promptly, reflecting the \\nurgency and prioritization inherent in healthcare settings.  \\nThe heap structure allows for constant-time retrieval of the patient with the highest priority, facilitating a \\nstreamlined triage process and optimizing the allocation of medical resources in emergency situations. \\n                \\n \\n                        Fig. 7: BASIC STRUCTURE OF MAX HEAP          Fig. 8: BASIC STRUCTURE OF MIN HEAP\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 7}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2354] \\nOOP \\nObject-Oriented Programming (OOP) is a programming paradigm that structures code around the concept of \\nobjects, which encapsulate data and behavior. OOP principles include encapsulation, where data and methods \\nare bundled within objects, enhancing code modularity. Inheritance allows the creation of new classes by \\ninheriting properties and methods from existing ones, promoting code reuse. Polymorphism enhances \\nflexibility by enabling objects of diverse types to be treated as instances of a shared type. Abstraction allows the \\nrepresentation of complex systems by focusing on essential properties and behaviors. OOP fosters code \\norganization, making it more intuitive and maintainable. Objects communicate through defined interfaces, \\nenhancing modularity and reducing dependencies. OOP is widely used in software development for modeling \\nreal-world entities and designing scalable, reusable, and modular code. Java, C++, and Python are popular OOP \\nlanguages. OOP promotes the design of software systems that reflect real-world structures and relationships. \\nHere\\'s an overview of these concepts and other aspects of Object-Oriented Programming:  \\n1. Objects and Classes:  \\nClass: A blueprint or template that defines the properties and behaviors common to all objects of a certain type.  \\nObject: An instance of a class, representing a concrete realization of the class blueprint.  \\n2. Encapsulation:  \\nEncapsulation involves bundling the data (attributes) and methods (functions) that operate on the data within \\na single unit, known as a class.  \\nThis shields the internal implementation details from the outside world, allowing the object to control access to \\nits data and methods.  \\n3. Inheritance:  \\nInheritance is a mechanism that allows a new class (subclass or derived class) to inherit attributes and \\nbehaviors from an existing class (superclass or base class).  \\nIt promotes code reusability and establishes an \"is-a\" relationship between the subclasses and the superclass.  \\n4. Polymorphism:  \\nPolymorphism permits the treatment of objects from various classes as instances of a common base class. It \\nincludes method overloading (multiple methods with the same name but different parameters) and method \\noverriding (providing a specific implementation for a method in a subclass).  \\n5. Abstraction:  \\nAbstraction simplifies intricate systems by modeling classes according to the essential properties and \\nbehaviors pertinent to the specific problem. It helps in managing software complexity by focusing on high-level \\nconcepts.  \\n6. Modularity:  \\nOOP promotes modularity by breaking down a system into smaller, independent, and interchangeable modules \\n(classes). Each module can be developed, tested, and maintained separately, contributing to a more organized \\nand scalable codebase.  \\n7. Encapsulation, Inheritance, and Polymorphism (EIP):  \\nTogether, encapsulation, inheritance, and polymorphism form the core principles of OOP, commonly known as \\nEIP. EIP provides a foundation for building flexible, maintainable, and extensible software systems.  \\n8. Class Relationships:  \\nAssociations: Connections between classes, such as one-to-one, one-to-many, and many to many relationships.  \\nAggregation: A type of association where one class represents a \"whole\" and another class represents a \"part.\"  \\nComposition: A stronger form of aggregation where the \"part\" is tightly bound to the \"whole.\"  \\n9. Design Patterns:  \\nDesign patterns are repetitive solutions addressing prevalent challenges in software design. OOP supports the \\nimplementation of design patterns, such as Singleton, Factory, Observer, and MVC (Model-View-Controller), to \\nimprove code organization and maintainability.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 8}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2355] \\nOOP is widely used in software development due to its ability to model real-world entities, enhance code \\norganization, and promote code reuse. Popular OOP languages include Java, C++, Python, and C#.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Banking System:  \\nIn an online banking system, Object-Oriented Programming is commonly employed to model entities such as \\naccounts, transactions, and customers. Each of these entities can be represented as objects with specific \\nproperties (account balance, transaction history) and behaviors (transfer funds, check balance). Inheritance can \\nbe used to represent different types of accounts, while encapsulation ensures that sensitive data is hidden and \\nonly accessible through defined methods.  \\n2. E-commerce Platform:  \\nE-commerce platforms utilize OOP to model products, orders, customers, and the shopping cart. Each product \\ncan be an object with properties like price and description. The shopping cart can be implemented as an object \\nthat manages the addition and removal of products. Inheritance can be applied to represent different types of \\nproducts or discounts, and polymorphism allows for a flexible checkout process.  \\n3. Video Game Development:  \\nIn the gaming industry, OOP is extensively used to model game entities like characters, enemies, weapons, and \\nenvironments. Each game object is represented as a class with specific attributes and behaviors. Inheritance \\ncan be employed to create variations of characters or enemies, and polymorphism allows for dynamic \\ninteractions between different game objects.  \\n4. Hospital Information System:  \\nHospital information systems often use OOP to model patients, doctors, appointments, and medical records. \\nEach entity can be represented as an object with relevant attributes and methods. Inheritance can be employed \\nto represent different types of medical professionals, and polymorphism can facilitate the integration of various \\nspecialized modules within the system.  \\n5. Social Media Platform:  \\n \\nFig. 9: ARCHITECTURE OF OOP \\nSocial media platforms extensively leverage OOP to model users, posts, comments, and interactions. Each user \\ncan be represented as an object with properties like username and profile information. Posts and comments \\ncan also be modeled as objects with associated behaviors. Inheritance can be applied to represent different'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 9}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2356] \\ntypes of users (regular users, administrators), and polymorphism allows for the dynamic handling of various \\ncontent types. These examples illustrate how Object-Oriented Programming is applied in diverse real-world \\nscenarios to model and manage complex systems by organizing code into modular and reusable structures. \\nOOP provides a scalable and maintainable approach to software development, enabling the creation of systems \\nthat mimic real-world entities and interactions. \\nDBMS \\nA Database Management System (DBMS) is a software tool designed to aid in the generation, administration, \\nand modification of databases. DBMS serves as an interface between the user and the database, providing tools \\nfor data definition, storage, retrieval, and manipulation. It ensures data integrity through features like \\nconstraints, enforcing rules on data stored in databases. DBMS supports the implementation of complex queries \\nand transactions, allowing users to interact with databases efficiently. It provides mechanisms for data security, \\nincluding user authentication, authorization, and encryption. DBMS allows for concurrent access by multiple \\nusers while maintaining consistency and isolation of data. Common types of DBMS include relational, NoSQL, \\nand object-oriented databases. SQL (Structured Query Language) is often used to interact with relational DBMS. \\nDBMS plays a crucial role in various applications, from business systems to web development, by providing a \\nstructured and efficient means of managing and accessing data.  \\nHere are key aspects of Database Management Systems:  \\n1. Data Definition Language (DDL):  \\nDDL is a subset of SQL (Structured Query Language) that allows users to define the structure of the database, \\nincluding creating, altering, and deleting tables and establishing relationships between them.  \\n2. Data Manipulation Language (DML):  \\nDML enables users to interact with the data stored in the database. Common DML operations include inserting, \\nupdating, and deleting records, as well as querying data using SELECT statements.  \\n3. Data Integrity:  \\nDBMS ensures data integrity by enforcing constraints such as primary keys, foreign keys, unique constraints, \\nand check constraints. These restrictions are in place to avoid the storage of data that is either invalid or \\ninconsistent in the database. \\n4. Concurrency Control:  \\nConcurrency control mechanisms in DBMS manage simultaneous access to data by multiple users to ensure \\ndata consistency. Techniques like locking and transaction isolation levels are employed to prevent conflicts.  \\n5. Transaction Management:  \\nDBMS supports transactions, which are sequences of one or more SQL operations treated as a single unit of \\nwork. Transactions ensure data consistency by either committing changes if successful or rolling back if an \\nerror occurs.  \\n6. Data Security:  \\nDBMS provides mechanisms for securing data, including user authentication, authorization, and access control. \\nUsers are granted specific permissions to perform operations on certain data, preventing unauthorized access.  \\n7. Data Modeling:  \\nDBMS enables the creation of data models to represent the structure and relationships within the database. \\nTypical data models encompass the relational model, hierarchical model, network model, and object-oriented \\nmodel. \\n8. Query Optimization:  \\nDBMS optimizes queries to enhance performance. Query optimization involves selecting the most efficient \\nexecution plan for a given query, considering factors like indexes, join algorithms, and access methods.  \\n9. Normalization:  \\nNormalization refers to the method of structuring data to minimize redundancy and enhance data integrity. \\nThis involves decomposing tables and ensuring that data dependencies are minimized.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 10}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2357] \\n10. Backup and Recovery:  \\nDBMS provides tools for creating backups of the database to prevent data loss in case of hardware failure, \\nsoftware errors, or other disasters. Recovery mechanisms restore the database to a consistent state after a \\nfailure.  \\n11. Scalability:  \\nDBMS systems are designed to scale, allowing for the management of large datasets and handling a growing \\nnumber of concurrent users. Scalability is crucial for accommodating expanding data requirements.  \\n12. Data Warehousing and Data Mining:  \\nDBMS is often integrated with data warehousing and data mining tools to support the extraction, \\ntransformation, and loading (ETL) of data, as well as the analysis and discovery of patterns and trends in large \\ndatasets.  \\nPopular Database Management Systems include MySQL, Oracle Database, Microsoft SQL Server, PostgreSQL, \\nand MongoDB. The choice of a DBMS depends on the specific requirements of the application, data model \\npreferences, and scalability needs.  \\nSome of the Real-World Applications of OOP are as follows:  \\n1. Online Retail and E-commerce Platforms:  \\nOnline retail platforms, such as Amazon and eBay, heavily rely on DBMS to manage vast product catalogs, \\ncustomer information, order processing, and inventory. The system ensures quick and accurate retrieval of \\nproduct information, tracks customer orders, and supports seamless transactions.  \\n2. Airline Reservation Systems:  \\nAirline reservation systems, like those used by airlines worldwide, utilize DBMS to manage flight schedules, \\nseat availability, passenger information, and ticketing. This ensures efficient booking processes, tracks \\npassenger details, and allows for dynamic adjustments to flight schedules.  \\n3. Human Resource Management Systems (HRMS):  \\nHRMS applications, used by companies for personnel management, leverage DBMS to store employee records, \\npayroll information, attendance data, and performance evaluations. The system ensures the secure and \\norganized management of HR-related information.  \\n4. Social Media Platforms:  \\nSocial media platforms, including Facebook, Twitter, and Instagram, employ DBMS to manage user profiles, \\nposts, comments, and social connections. The system enables rapid retrieval of personalized content, tracks \\nuser interactions, and supports features like recommendations and targeted advertising.  \\n5. Library Management Systems:  \\nLibrary management systems, used by educational institutions and public libraries, utilize DBMS to organize \\nand catalog books, manage borrower information, and track lending transactions. The system ensures efficient \\nlibrary operations, including book searches, checkouts, and returns.  \\nThese examples showcase the diverse applications of DBMS in different industries, highlighting its role in \\norganizing, storing, and retrieving data to support critical business functions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 11}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2358] \\n \\nFig. 10: ARCHITECTURE OF DBMS \\nOS \\nAn Operating System (OS) is a software that acts as an intermediary between computer hardware and user \\napplications, managing hardware resources and facilitating seamless execution of software. It provides \\nessential services like process management, handling the creation, scheduling, and termination of processes. \\nMemory management allocates and deallocates memory space for processes, ensuring efficient utilization. File \\nsystem management organizes and stores data on storage devices, allowing for file creation, deletion, and \\nmanipulation. Device management facilitates communication between the OS and hardware devices, such as \\ninput/output peripherals. Security features include user authentication, access controls, and encryption to \\nsafeguard the system. The OS provides a user interface (UI), which can be command-line or graphical, enabling \\nuser interaction. Networking capabilities support communication between computers and devices within a \\nnetwork. Error handling mechanisms address issues and maintain system stability. The OS is fundamental for \\nthe execution of diverse software applications and is integral to the functionality of computers and devices.  \\nHere are key aspects of Operating Systems:  \\n1. Kernel:  \\nThe kernel constitutes the central element of the operating system. It provides essential services, including \\nprocess management, memory management, device management, and system calls. The kernel interacts \\ndirectly with the hardware and ensures that different software components can run efficiently.  \\n2. Process Management:  \\nProcess management encompasses the initiation, scheduling, and cessation of processes. The OS manages the \\nexecution of multiple processes concurrently, allowing users to run multiple applications simultaneously.  \\n3. Memory Management:  \\nMemory management is responsible for allocating and deallocating memory space for processes. The OS \\nensures efficient utilization of memory, handles memory protection, and facilitates virtual memory to extend \\navailable RAM.  \\n4. File System Management:  \\nThe OS manages file systems, organizing and storing data on storage devices such as hard drives. It provides \\nfile-related services, including file creation, deletion, reading, and writing. File systems organize data into \\ndirectories and files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 12}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2359] \\n5. Device Management:  \\nDevice management handles communication between the OS and hardware devices, including input/output \\ndevices like keyboards, printers, and storage devices. It provides drivers and interfaces to enable \\ncommunication and control of these devices.  \\n6. Security and Protection:  \\nThe operating system enforces security protocols to safeguard both the system and user data. This includes \\nuser authentication, access controls, encryption, and security patches to address vulnerabilities. The OS also \\nenforces memory protection to prevent unauthorized access to memory areas.  \\n7. User Interface:  \\nThe user interface (UI) provides a means for users to interact with the computer. OS can have a command-line \\ninterface (CLI) or a graphical user interface (GUI). GUIs include elements like windows, icons, menus, and \\nbuttons to enhance user experience.  \\n8. Networking:  \\nNetworking capabilities enable computers to communicate with each other in a network. The OS provides \\nnetworking protocols and services, facilitating tasks such as file sharing, internet access, and remote access to \\nresources.  \\n9. Schedulers:  \\nSchedulers manage the execution of processes and allocate CPU time. They include process schedulers for \\nshort-term CPU scheduling, and long-term schedulers for selecting processes to be brought into the ready \\nqueue.  \\n10. Error Handling:  \\nOS handles errors and exceptions to maintain system stability. It provides error messages, logs, and recovery \\nmechanisms to address issues that may arise during system operation.  \\n11. System Calls:  \\nSystem calls are interfaces through which user-level programs request services from the operating system. \\nCommon system calls include file operations, process control, and memory management.  \\n12. Boot Process:  \\nThe boot process initializes the computer's hardware and loads the OS into memory. It involves the BIOS/UEFI, \\nbootloader, and kernel, leading to the full initialization of the OS.  \\nOperating systems come in various types, including Windows, macOS, Linux, Unix, and others. Each serve as a \\nfundamental layer in the computing stack, enabling the execution of applications and providing a user-friendly \\nenvironment for users to interact with their devices.  \\nHere are two examples of real-world applications that extensively rely on Operating Systems:  \\n1. Automated Teller Machines (ATMs):  \\nAutomated Teller Machines, commonly found in banks and other financial institutions, heavily depend on \\noperating systems for their functionality. The OS manages interactions between the ATM hardware \\ncomponents, such as the card reader, cash dispenser, and keypad. It handles security features, user \\nauthentication, and communication with the bank's servers. The OS ensures that transactions are processed \\nsecurely and efficiently, providing users with a seamless and reliable experience.  \\n2. Smartphones:  \\nSmartphones, including those running iOS (Apple), Android, or other mobile operating systems, are ubiquitous \\ndevices that rely on operating systems to manage a wide range of functions. The OS on a smartphone handles \\nthe user interface, application management, memory allocation, power management, and communication with \\nvarious hardware components (e.g., camera, GPS, sensors). It ensures a smooth user experience by coordinating \\nthe execution of diverse applications and services, making smartphones an integral part of everyday life. These \\nexamples illustrate how operating systems play a crucial role in diverse technological applications, enabling the \\neffective and secure functioning of complex systems.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 13}, page_content=\"e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2360] \\n \\nFig. 11: ARCHITECTURE OF OS \\nIII. \\nMETHODOLOGY \\nIn the landscape of computer science, the integration and interconnectivity among Data Structures and \\nAlgorithms (DSA), Object-Oriented Programming (OOP), Database Management Systems (DBMS), and \\nOperating Systems (OS) form the backbone of sophisticated software development and system design. Each of \\nthese fundamental subjects contributes distinct elements to the overall architecture of a computing system, and \\ntheir seamless integration is essential for creating robust and efficient applications.  \\nHere's an exploration of how these subjects intersect:  \\n1. DSA in System Design:  \\nData Structures and Algorithms play a pivotal role in designing efficient and scalable systems. DSA is the \\nbedrock for optimizing data storage, retrieval, and manipulation. Algorithms, whether for sorting, searching, or \\noptimizing, are crucial for creating performant applications. In conjunction with DSA, system architects can \\ndesign data models that align with the requirements of both DBMS and OOP, ensuring effective utilization of \\nresources.  \\n2. OOP for Code Organization and Reusability:  \\nObject-Oriented Programming provides a paradigm for organizing code into modular and reusable \\ncomponents. Objects encapsulate data and behavior, facilitating the creation of software entities that model \\nreal-world entities. In an integrated system, OOP principles enable clean interfaces and interactions between \\ndifferent modules. DSA structures can be seamlessly integrated into OOP-designed systems, allowing for \\nefficient data manipulation and algorithm execution within the context of object-oriented applications.  \\n3. DBMS for Persistent Data Storage:  \\nDatabase Management Systems are central to storing and retrieving data persistently. In an integrated system, \\nthe interaction between DSA and DBMS is evident in how data structures are translated into database schemas.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 14}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2361] \\nDSA concepts like indexing and normalization directly impact how data is organized in a database. The system\\'s \\narchitecture, shaped by DSA, ensures that data is efficiently managed and manipulated through DBMS, \\nproviding a structured and scalable approach to data storage.  \\n4. OS for Resource Management and Execution:  \\nThe Operating System acts as the underlying platform that orchestrates the execution of software applications. \\nIn an integrated system, the OS interacts with applications designed using OOP principles, allocates memory \\nbased on DSA requirements, and manages access to data stored in DBMS. The OS ensures the efficient \\nutilization of hardware resources, providing a foundation for the execution of diverse software components.  \\nExamples of Interconnectivity:  \\n1. E-commerce Platform:  \\nAn e-commerce platform exemplifies the interconnectivity of DSA, OOP, DBMS, and OS. DSA algorithms are \\nemployed for efficient search and recommendation systems. OOP principles are applied for organizing code \\ninto modular components. DBMS is used to store and retrieve product information, user data, and transaction \\nrecords. The OS ensures the secure and efficient execution of the entire system.  \\n2. Healthcare Information System:  \\nIn a healthcare information system, DSA is used for optimizing patient data structures, OOP principles guide the \\norganization of healthcare modules, DBMS manages patient records, and the OS ensures the secure and reliable \\noperation of the entire system. The interplay of these fundamental subjects is crucial for maintaining the \\nintegrity and efficiency of healthcare information systems. In essence, the integration of DSA, OOP, DBMS, and \\nOS is not merely a theoretical concept but a practical necessity for building sophisticated, interconnected, and \\nhigh-performance computing systems. Understanding how these subjects complement each other is essential \\nfor professionals and students alike, as it lays the groundwork for effective problem-solving and innovation in \\nthe ever-evolving landscape of computer science. \\nIV. \\nCONCLUSION \\nIn conclusion, this paper underscores the foundational significance of Data Structures and Algorithms (DSA), \\nObject-Oriented Programming (OOP), Database Management Systems (DBMS), and Operating Systems (OS) in \\nthe realm of computer science. The paper emphasizes that a strong and comprehensive understanding of these \\nfundamental subjects is not only beneficial but essential for success in the dynamic and ever-evolving field of \\ncomputer science. Whether for students embarking on their educational journey or professionals navigating the \\nintricacies of software development, the knowledge gained in DSA, OOP, DBMS, and OS serves as a bedrock for \\nbuilding innovative solutions, tackling challenges, and contributing meaningfully to the advancement of \\ntechnology. \\nV. \\nREFERENCES \\n[1] \\nR. K. Vosilovich, \"Universal Linear Data Structure,\" 2020 International Conference on Information \\nScience and Communications Technologies (ICISCT), Tashkent, Uzbekistan, 2020, pp. 1-3, doi: \\n10.1109/ICISCT50599.2020.9351485.  \\n[2] \\nDonald Knuth. The Art of Computer Programming, Volume 1: Fundamental Algorithms, Third Edition. \\nAddison-Wesley, 1997. ISBN 0-201-89683-4. Section 2.2.1: Stacks, Queues, and Deques, pp. 238–243.  \\n[3] \\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to \\nAlgorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 10.1: \\nStacks and queues, pp. 200–204.  \\n[4] \\nE. Vaahedi and K. W. Cheung, \"Evolution and future of on-line DSA,\" 2000 IEEE Power Engineering \\nSociety Winter Meeting. Conference Proceedings (Cat. No.00CH37077), Singapore, 2000, pp. 63-65 \\nvol.1, doi: 10.1109/PESW.2000.849928.  \\n[5] \\nT. Mudner and E. Shakshuki, \"A new approach to learning algorithms,\" International Conference on \\nInformation Technology: Coding and Computing, 2004. Proceedings. ITCC 2004., Las Vegas, NV, USA, \\n2004, pp. 141-145 Vol.1, doi: 10.1109/ITCC.2004.1286440.  \\n[6] \\nSummer Meeting. Conference Proceedings (Cat. No.01CH37262), Vancouver, BC, Canada, 2001, pp. \\n1070-1074 vol.2, doi: 10.1109/PESS.2001.970207.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2023-12-24T15:53:53+05:30', 'source': '../data/pdf_files/cs_concepts.pdf', 'file_path': '../data/pdf_files/cs_concepts.pdf', 'total_pages': 16, 'format': 'PDF 1.5', 'title': '', 'author': 'Rampyari', 'subject': '', 'keywords': '', 'moddate': '2023-12-24T15:53:53+05:30', 'trapped': '', 'modDate': \"D:20231224155353+05'30'\", 'creationDate': \"D:20231224155353+05'30'\", 'page': 15}, page_content='e-ISSN: 2582-5208 \\nInternational Research  Journal  of  Modernization in Engineering  Technology  and Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:05/Issue:12/December-2023                  Impact Factor- 7.868                           www.irjmets.com                       \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [2362] \\n[7] \\nB. Stroustrup, \"What is object-oriented programming?,\" in IEEE Software, vol. 5, no. 3, pp. 1020, May \\n1988, doi: 10.1109/52.2020.  \\n[8] \\nG. Schlageter et al., \"OOPS-an object oriented programming system with integrated data management \\nfacility,\" Proceedings. Fourth International Conference on Data Engineering, Los Angeles, CA, USA, \\n1988, pp. 118-125, doi: 10.1109/ICDE.1988.105453. \\n[9] \\nS. Abbasi, H. Kazi and K. Khowaja, \"A systematic review of learning object oriented programming \\nthrough serious games and programming approaches,\" 2017 4th IEEE International Conference on \\nEngineering Technologies and Applied Sciences (ICETAS), Salmabad, Bahrain, 2017, pp. 1-6,  \\ndoi: 10.1109/ICETAS.2017.8277894. \\n[10] \\nR. J. D\\'Andrea and R. G. Gowda, \"Object-oriented programming: concepts and languages,\" IEEE \\nConference on Aerospace and Electronics, Dayton, OH, USA, 1990, pp. 634-640 vol.2,  \\ndoi: 10.1109/NAECON.1990.112840.  \\n[11] \\nS. Samaiya and M. Agarwal, \"Real time database management system,\" 2018 2nd International \\nConference on Inventive Systems and Control (ICISC), Coimbatore, India, 2018, pp. 903-908,  \\ndoi: 10.1109/ICISC.2018.8398931.  \\n[12] \\nWiederhold, \"Databases,\" in Computer, vol. 17, no. 10, pp. 211-223, Oct. 1984,  \\ndoi: 10.1109/MC.1984.1658971.  \\n[13] \\nP. G. Selinger, \"Database technology,\" in IBM Systems Journal, vol. 26, no. 1, pp. 96-106, 1987,  \\ndoi: 10.1147/sj.261.0096.  \\n[14] \\nA. Corallo, M. Espostito, A. Massafra and S. Totaro, \"A Relational Database Management System \\nApproach for Data Integration in Manufacturing Process,\" 2018 IEEE International Conference on \\nEngineering, Technology and Innovation (ICE/ITMC), Stuttgart, Germany, 2018, pp. 1-7,  \\ndoi: 10.1109/ICE.2018.8436290.  \\n[15] \\nS. H. Son, \"Real-time database systems: present and future,\" Proceedings Second International \\nWorkshop on Real-Time Computing Systems and Applications, Tokyo, Japan, 1995, pp. 50-52,  \\ndoi: 10.1109/RTCSA.1995.528750.  \\n[16] \\nR. R. Muntz, \"Operating systems,\" in Computer, vol. 7, no. 6, pp. 21-21, June 1974,  \\ndoi: 10.1109/MC.1974.6323579.  \\n[17] \\nW. Chengjun, \"The Analyses of Operating System Structure,\" 2009 Second International Symposium on \\nKnowledge Acquisition and Modeling, Wuhan, China, 2009, pp. 354-357, doi: 10.1109/KAM.2009.265.  \\n[18] \\nNorman F. Schneidewind, \"Operating Systems,\" in Computer, Network, Software, and Hardware \\nEngineering with Applications , IEEE, 2012, pp.286-302, doi: 10.1002/9781118181287.ch10.  \\n[19] \\nH. Mei and Y. Guo, \"Operating Systems for Internetware: Challenges and Future Directions,\" 2018 IEEE \\n38th International Conference on Distributed Computing Systems (ICDCS), Vienna, Austria, 2018, pp. \\n1377-1384, doi: 10.1109/ICDCS.2018.00138.  \\n[20] \\nC. Peng, X. -q. Yang, Z. -z. Niu and X. -p. Liu, \"Research on Windows operating system education,\" 2009 \\nIEEE International Symposium on IT in Medicine & Education, Jinan, China, 2009, pp. 719-724, doi: \\n10.1109/ITIME.2009.5236327.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "pdf_dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls= PyMuPDFLoader,\n",
    "    show_progress=False,\n",
    ")\n",
    "\n",
    "pdf_docs = pdf_dir_loader.load()\n",
    "pdf_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e297e5",
   "metadata": {},
   "source": [
    "### Directory Loader - For CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "csv_loader = DirectoryLoader(\n",
    "    \"../data/csv_files\",\n",
    "    glob=\"**/*.csv\",\n",
    "    loader_cls= CSVLoader,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "csv_docs = csv_loader.load()\n",
    "csv_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3def72",
   "metadata": {},
   "source": [
    "### Directory Loader - For Docx files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd1b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file ../data/docx_files/sample3(1).docx\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docx2txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword_document\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Docx2txtLoader\n\u001b[1;32m      4\u001b[0m docx_dir_loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/docx_files\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     loader_cls\u001b[38;5;241m=\u001b[39m Docx2txtLoader,\n\u001b[1;32m      8\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m docx \u001b[38;5;241m=\u001b[39m \u001b[43mdocx_dir_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m docx\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[0;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[1;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:233\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/directory.py:223\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[0;34m(self, item, path, pbar)\u001b[0m\n\u001b[1;32m    221\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdoc \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m subdoc\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:95\u001b[0m, in \u001b[0;36mBaseLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A lazy loader for `Document`.\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03mYields:\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    The `Document` objects.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mload \u001b[38;5;241m!=\u001b[39m BaseLoader\u001b[38;5;241m.\u001b[39mload:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     96\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not implement lazy_load()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[0;32m/mnt/5d9f5ec4-97b1-4495-8630-aafd5bd9fb5d/rag/.venv/lib/python3.10/site-packages/langchain_community/document_loaders/word_document.py:57\u001b[0m, in \u001b[0;36mDocx2txtLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m     56\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load given path as single page.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdocx2txt\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     60\u001b[0m         Document(\n\u001b[1;32m     61\u001b[0m             page_content\u001b[38;5;241m=\u001b[39mdocx2txt\u001b[38;5;241m.\u001b[39mprocess(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path),\n\u001b[1;32m     62\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_file_path},\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m     ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx2txt'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
    "\n",
    "docx_dir_loader = DirectoryLoader(\n",
    "    \"../data/docx_files\",\n",
    "    glob=\"**/*.docx\",\n",
    "    loader_cls= Docx2txtLoader,\n",
    "    show_progress=False,\n",
    ")\n",
    "\n",
    "docx = docx_dir_loader.load()\n",
    "docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
